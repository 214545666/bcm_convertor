{"version":3,"sources":["webpack:///webpack/bootstrap","webpack:///./node_modules/convnetjs/build/convnet.js","webpack:///./node_modules/@cmao/codemao-ai/dist/es6/ai_logics/defs.js","webpack:///./node_modules/@cmao/codemao-ai/dist/es6/ai_logics/classify_ai/classify_controller/worker.js"],"names":["installedModules","__webpack_require__","moduleId","exports","module","i","l","modules","call","m","c","d","name","getter","o","Object","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","prototype","hasOwnProperty","p","s","convnetjs","REVISION","global","return_v","v_val","gaussRandom","u","Math","random","v","sqrt","log","randf","a","b","arrContains","arr","elt","length","randi","floor","randn","mu","std","zeros","isNaN","ArrayBuffer","Array","Float64Array","maxmin","w","maxv","minv","maxi","mini","dv","randperm","temp","j","array","q","weightedSample","lst","probs","cumprob","k","arrUnique","push","getopt","opt","field_name","default_value","Vol","sx","sy","depth","toString","this","dw","scale","x","y","ix","set","add","get_grad","set_grad","add_grad","cloneAndZero","clone","V","addFrom","addFromScaled","setConst","toJSON","json","fromJSON","augment","crop","dx","dy","fliplr","W","W2","img_to_vol","img","convert_grayscale","canvas","document","createElement","width","height","ctx","getContext","drawImage","e","img_data","getImageData","data","H","pv","x1","ConvLayer","out_depth","filters","in_depth","in_sx","in_sy","stride","pad","l1_decay_mul","l2_decay_mul","out_sx","out_sy","layer_type","bias","bias_pref","biases","forward","is_training","in_act","A","f","ax","ay","fx","fy","fd","oy","ox","out_act","backward","chain_grad","ix1","ix2","getParamsAndGrads","response","params","grads","FullyConnLayer","num_neurons","num_inputs","Vw","wi","tfi","PoolLayer","switchx","switchy","winx","winy","InputLayer","SoftmaxLayer","as","amax","es","esum","exp","mul","RegressionLayer","loss","dim","yi","val","SVMLayer","yscore","ReluLayer","V2","N","V2w","SigmoidLayer","v2wi","MaxoutLayer","group_size","switches","ai","a2","TanhLayer","DropoutLayer","drop_prob","dropped","LocalResponseNormalizationLayer","alpha","beta","console","S_cache_","n2","den","max","min","aa","pow","S","SB","SB2","aj","g","QuadTransformLayer","Ni","i0","i1","Net","options","layers","makeLayers","defs","type","new_defs","def","num_classes","activation","tensor","gs","desugar","prev","act","layer_reponse","getPrediction","L","Lj","Trainer","net","learning_rate","l1_decay","l2_decay","batch_size","method","momentum","ro","eps","gsum","xsum","train","start","Date","getTime","fwd_time","cost_loss","l2_decay_loss","l1_decay_loss","bwd_time","pglist","pg","plen","abs","l1grad","gij","gsumi","xsumi","softmax_loss","SGDTrainer","MagicNet","labels","train_ratio","num_folds","num_candidates","num_epochs","ensemble_size","batch_size_min","batch_size_max","l2_decay_min","l2_decay_max","learning_rate_min","learning_rate_max","momentum_min","momentum_max","neurons_min","neurons_max","folds","candidates","evaluated_candidates","unique_labels","iter","foldix","finish_fold_callback","finish_batch_callback","sampleFolds","sampleCandidates","num_train","train_ix","slice","test_ix","sampleCandidate","input_depth","layer_defs","nl","ni","dp","trainer_def","bs","l2","lr","mom","tp","trainer","cand","step","fold","dataix","lastiter","val_acc","evalValErrors","acc","accv","sort","vals","predict_soft","xout","nv","predict","predicted_label","nets","dummy_candidate","onFinishFold","onFinishBatch","lib","window","jsfeat","RANGE_TRAINING_DATA","self","sendMessage","postMessage","addEventListener","message","receive_data","JSON","parse","send_data","train_net","convnet","model","input_array","input","output_array","output","train_max","train_min_1","train_length_1","forEach","item","index","dimension","loop_time","train_data","id","draw_net","draw_data","num_line","Number","toFixed","_loop_1","test_node","scores","main_class","benchmark","density","is_simple_mode","weight","class_index","unshift"],"mappings":"aACE,IAAIA,EAAmB,GAGvB,SAASC,EAAoBC,GAG5B,GAAGF,EAAiBE,GACnB,OAAOF,EAAiBE,GAAUC,QAGnC,IAAIC,EAASJ,EAAiBE,GAAY,CACzCG,EAAGH,EACHI,GAAG,EACHH,QAAS,IAUV,OANAI,EAAQL,GAAUM,KAAKJ,EAAOD,QAASC,EAAQA,EAAOD,QAASF,GAG/DG,EAAOE,GAAI,EAGJF,EAAOD,QAKfF,EAAoBQ,EAAIF,EAGxBN,EAAoBS,EAAIV,EAGxBC,EAAoBU,EAAI,SAASR,EAASS,EAAMC,GAC3CZ,EAAoBa,EAAEX,EAASS,IAClCG,OAAOC,eAAeb,EAASS,EAAM,CAAEK,YAAY,EAAMC,IAAKL,KAKhEZ,EAAoBkB,EAAI,SAAShB,GACX,oBAAXiB,QAA0BA,OAAOC,aAC1CN,OAAOC,eAAeb,EAASiB,OAAOC,YAAa,CAAEC,MAAO,WAE7DP,OAAOC,eAAeb,EAAS,aAAc,CAAEmB,OAAO,KAQvDrB,EAAoBsB,EAAI,SAASD,EAAOE,GAEvC,GADU,EAAPA,IAAUF,EAAQrB,EAAoBqB,IAC/B,EAAPE,EAAU,OAAOF,EACpB,GAAW,EAAPE,GAA8B,iBAAVF,GAAsBA,GAASA,EAAMG,WAAY,OAAOH,EAChF,IAAII,EAAKX,OAAOY,OAAO,MAGvB,GAFA1B,EAAoBkB,EAAEO,GACtBX,OAAOC,eAAeU,EAAI,UAAW,CAAET,YAAY,EAAMK,MAAOA,IACtD,EAAPE,GAA4B,iBAATF,EAAmB,IAAI,IAAIM,KAAON,EAAOrB,EAAoBU,EAAEe,EAAIE,EAAK,SAASA,GAAO,OAAON,EAAMM,IAAQC,KAAK,KAAMD,IAC9I,OAAOF,GAIRzB,EAAoB6B,EAAI,SAAS1B,GAChC,IAAIS,EAAST,GAAUA,EAAOqB,WAC7B,WAAwB,OAAOrB,EAAgB,SAC/C,WAA8B,OAAOA,GAEtC,OADAH,EAAoBU,EAAEE,EAAQ,IAAKA,GAC5BA,GAIRZ,EAAoBa,EAAI,SAASiB,EAAQC,GAAY,OAAOjB,OAAOkB,UAAUC,eAAe1B,KAAKuB,EAAQC,IAGzG/B,EAAoBkC,EAAI,IAIjBlC,EAAoBA,EAAoBmC,EAAI,G,kBClFrD,IAAIC,EAAYA,GAAa,CAAEC,SAAU,UACzC,SAAUC,GACR,aAGA,IAAIC,GAAW,EACXC,EAAQ,EACRC,EAAc,WAChB,GAAGF,EAED,OADAA,GAAW,EACJC,EAET,IAAIE,EAAI,EAAEC,KAAKC,SAAS,EACpBC,EAAI,EAAEF,KAAKC,SAAS,EACpB1B,EAAIwB,EAAEA,EAAIG,EAAEA,EAChB,GAAQ,GAAL3B,GAAUA,EAAI,EAAG,OAAOuB,IAC3B,IAAIhC,EAAIkC,KAAKG,MAAM,EAAEH,KAAKI,IAAI7B,GAAGA,GAGjC,OAFAsB,EAAQK,EAAEpC,EACV8B,GAAW,EACJG,EAAEjC,GAEPuC,EAAQ,SAASC,EAAGC,GAAK,OAAOP,KAAKC,UAAUM,EAAED,GAAGA,GAiBpDE,EAAc,SAASC,EAAKC,GAC9B,IAAI,IAAIjD,EAAE,EAAEyB,EAAEuB,EAAIE,OAAOlD,EAAEyB,EAAEzB,IAC3B,GAAGgD,EAAIhD,KAAKiD,EAAK,OAAO,EAE1B,OAAO,GA4DTf,EAAOU,MAAQA,EACfV,EAAOiB,MAjFK,SAASN,EAAGC,GAAK,OAAOP,KAAKa,MAAMb,KAAKC,UAAUM,EAAED,GAAGA,IAkFnEX,EAAOmB,MAjFK,SAASC,EAAIC,GAAM,OAAOD,EAAGjB,IAAckB,GAkFvDrB,EAAOsB,MA/EK,SAAS/B,GACnB,QAAe,IAAN,GAAqBgC,MAAMhC,GAAM,MAAO,GACjD,GAA0B,oBAAhBiC,YAA6B,CAGrC,IADA,IAAIV,EAAM,IAAIW,MAAMlC,GACZzB,EAAE,EAAEA,EAAEyB,EAAEzB,IAAOgD,EAAIhD,GAAI,EAC/B,OAAOgD,EAEP,OAAO,IAAIY,aAAanC,IAwE5BS,EAAO2B,OAlDM,SAASC,GACpB,GAAgB,IAAbA,EAAEZ,OAAgB,MAAO,GAM5B,IALA,IAAIa,EAAOD,EAAE,GACTE,EAAOF,EAAE,GACTG,EAAO,EACPC,EAAO,EACPzC,EAAIqC,EAAEZ,OACFlD,EAAE,EAAEA,EAAEyB,EAAEzB,IACX8D,EAAE9D,GAAK+D,IAAQA,EAAOD,EAAE9D,GAAIiE,EAAOjE,GACnC8D,EAAE9D,GAAKgE,IAAQA,EAAOF,EAAE9D,GAAIkE,EAAOlE,GAExC,MAAO,CAACiE,KAAMA,EAAMF,KAAMA,EAAMG,KAAMA,EAAMF,KAAMA,EAAMG,GAAGJ,EAAKC,IAwClE9B,EAAOkC,SApCQ,SAAS3C,GAKtB,IAJA,IAEI4C,EAFArE,EAAIyB,EACJ6C,EAAI,EAEJC,EAAQ,GACJC,EAAE,EAAEA,EAAE/C,EAAE+C,IAAID,EAAMC,GAAGA,EAC7B,KAAOxE,KACHsE,EAAI/B,KAAKa,MAAMb,KAAKC,UAAYxC,EAAE,IAClCqE,EAAOE,EAAMvE,GACbuE,EAAMvE,GAAKuE,EAAMD,GACjBC,EAAMD,GAAKD,EAEf,OAAOE,GAyBTrC,EAAOuC,eApBc,SAASC,EAAKC,GAGjC,IAFA,IAAI7C,EAAIc,EAAM,EAAG,GACbgC,EAAU,EACNC,EAAE,EAAEpD,EAAEiD,EAAIxB,OAAO2B,EAAEpD,EAAEoD,IAE3B,GAAG/C,GADH8C,GAAWD,EAAME,IACC,OAAOH,EAAIG,IAgBjC3C,EAAO4C,UAhES,SAAS9B,GAEvB,IADA,IAAIF,EAAI,GACA9C,EAAE,EAAEyB,EAAEuB,EAAIE,OAAOlD,EAAEyB,EAAEzB,IACvB+C,EAAYD,EAAGE,EAAIhD,KACrB8C,EAAEiC,KAAK/B,EAAIhD,IAGf,OAAO8C,GA0DTZ,EAAOa,YAAcA,EACrBb,EAAO8C,OAbM,SAASC,EAAKC,EAAYC,GACrC,YAAkC,IAApBF,EAAIC,GAA8BD,EAAIC,GAAcC,GAlGtE,CAgHGnD,GACH,SAAUE,GACR,aASA,IAAIkD,EAAM,SAASC,EAAIC,EAAIC,EAAOlF,GAEhC,GAA0C,mBAAvCK,OAAOkB,UAAU4D,SAASrF,KAAKkF,GAA0B,CAE1DI,KAAKJ,GAAK,EACVI,KAAKH,GAAK,EACVG,KAAKF,MAAQF,EAAGnC,OAGhBuC,KAAK3B,EAAI5B,EAAOsB,MAAMiC,KAAKF,OAC3BE,KAAKC,GAAKxD,EAAOsB,MAAMiC,KAAKF,OAC5B,IAAI,IAAIvF,EAAE,EAAEA,EAAEyF,KAAKF,MAAMvF,IACvByF,KAAK3B,EAAE9D,GAAKqF,EAAGrF,OAEZ,CAELyF,KAAKJ,GAAKA,EACVI,KAAKH,GAAKA,EACVG,KAAKF,MAAQA,EACb,IAAI9D,EAAI4D,EAAGC,EAAGC,EAGd,GAFAE,KAAK3B,EAAI5B,EAAOsB,MAAM/B,GACtBgE,KAAKC,GAAKxD,EAAOsB,MAAM/B,QACP,IAANpB,EAIR,KAAIsF,EAAQpD,KAAKG,KAAK,GAAK2C,EAAGC,EAAGC,IACjC,IAAQvF,EAAE,EAAEA,EAAEyB,EAAEzB,IACdyF,KAAK3B,EAAE9D,GAAKkC,EAAOmB,MAAM,EAAKsC,QAGhC,IAAQ3F,EAAE,EAAEA,EAAEyB,EAAEzB,IACdyF,KAAK3B,EAAE9D,GAAKK,IAMpB+E,EAAIxD,UAAY,CACdf,IAAK,SAAS+E,EAAGC,EAAGvF,GAClB,IAAIwF,GAAKL,KAAKJ,GAAKQ,EAAGD,GAAGH,KAAKF,MAAMjF,EACpC,OAAOmF,KAAK3B,EAAEgC,IAEhBC,IAAK,SAASH,EAAGC,EAAGvF,EAAGmC,GACrB,IAAIqD,GAAKL,KAAKJ,GAAKQ,EAAGD,GAAGH,KAAKF,MAAMjF,EACpCmF,KAAK3B,EAAEgC,GAAMrD,GAEfuD,IAAK,SAASJ,EAAGC,EAAGvF,EAAGmC,GACrB,IAAIqD,GAAKL,KAAKJ,GAAKQ,EAAGD,GAAGH,KAAKF,MAAMjF,EACpCmF,KAAK3B,EAAEgC,IAAOrD,GAEhBwD,SAAU,SAASL,EAAGC,EAAGvF,GACvB,IAAIwF,GAAOL,KAAKJ,GAAKQ,EAAGD,GAAGH,KAAKF,MAAMjF,EACtC,OAAOmF,KAAKC,GAAGI,IAEjBI,SAAU,SAASN,EAAGC,EAAGvF,EAAGmC,GAC1B,IAAIqD,GAAOL,KAAKJ,GAAKQ,EAAGD,GAAGH,KAAKF,MAAMjF,EACtCmF,KAAKC,GAAGI,GAAMrD,GAEhB0D,SAAU,SAASP,EAAGC,EAAGvF,EAAGmC,GAC1B,IAAIqD,GAAOL,KAAKJ,GAAKQ,EAAGD,GAAGH,KAAKF,MAAMjF,EACtCmF,KAAKC,GAAGI,IAAOrD,GAEjB2D,aAAc,WAAa,OAAO,IAAIhB,EAAIK,KAAKJ,GAAII,KAAKH,GAAIG,KAAKF,MAAO,IACxEc,MAAO,WAGL,IAFA,IAAIC,EAAI,IAAIlB,EAAIK,KAAKJ,GAAII,KAAKH,GAAIG,KAAKF,MAAO,GAC1C9D,EAAIgE,KAAK3B,EAAEZ,OACPlD,EAAE,EAAEA,EAAEyB,EAAEzB,IAAOsG,EAAExC,EAAE9D,GAAKyF,KAAK3B,EAAE9D,GACvC,OAAOsG,GAETC,QAAS,SAASD,GAAK,IAAI,IAAIzB,EAAE,EAAEA,EAAEY,KAAK3B,EAAEZ,OAAO2B,IAAOY,KAAK3B,EAAEe,IAAMyB,EAAExC,EAAEe,IAC3E2B,cAAe,SAASF,EAAGzD,GAAK,IAAI,IAAIgC,EAAE,EAAEA,EAAEY,KAAK3B,EAAEZ,OAAO2B,IAAOY,KAAK3B,EAAEe,IAAMhC,EAAEyD,EAAExC,EAAEe,IACtF4B,SAAU,SAAS5D,GAAK,IAAI,IAAIgC,EAAE,EAAEA,EAAEY,KAAK3B,EAAEZ,OAAO2B,IAAOY,KAAK3B,EAAEe,GAAKhC,GAEvE6D,OAAQ,WAEN,IAAIC,EAAO,GAKX,OAJAA,EAAKtB,GAAKI,KAAKJ,GACfsB,EAAKrB,GAAKG,KAAKH,GACfqB,EAAKpB,MAAQE,KAAKF,MAClBoB,EAAK7C,EAAI2B,KAAK3B,EACP6C,GAGTC,SAAU,SAASD,GACjBlB,KAAKJ,GAAKsB,EAAKtB,GACfI,KAAKH,GAAKqB,EAAKrB,GACfG,KAAKF,MAAQoB,EAAKpB,MAElB,IAAI9D,EAAIgE,KAAKJ,GAAGI,KAAKH,GAAGG,KAAKF,MAC7BE,KAAK3B,EAAI5B,EAAOsB,MAAM/B,GACtBgE,KAAKC,GAAKxD,EAAOsB,MAAM/B,GAEvB,IAAI,IAAIzB,EAAE,EAAEA,EAAEyB,EAAEzB,IACdyF,KAAK3B,EAAE9D,GAAK2G,EAAK7C,EAAE9D,KAKzBkC,EAAOkD,IAAMA,EA7Gf,CA8GGpD,GACH,SAAUE,GACR,aACA,IAAIkD,EAAMlD,EAAOkD,IAsGjBlD,EAAO2E,QA/FO,SAASP,EAAGQ,EAAMC,EAAIC,EAAIC,GAEtC,QAAoB,IAAX,EAA4BA,GAAS,EAC9C,QAAgB,IAAP,EAAwBF,EAAK7E,EAAOiB,MAAM,EAAGmD,EAAEjB,GAAKyB,GAC7D,QAAgB,IAAP,EAAwBE,EAAK9E,EAAOiB,MAAM,EAAGmD,EAAEhB,GAAKwB,GAG7D,IAAII,EACJ,GAAGJ,IAASR,EAAEjB,IAAW,IAAL0B,GAAe,IAALC,EAAQ,CACpCE,EAAI,IAAI9B,EAAI0B,EAAMA,EAAMR,EAAEf,MAAO,GACjC,IAAI,IAAIK,EAAE,EAAEA,EAAEkB,EAAKlB,IACjB,IAAI,IAAIC,EAAE,EAAEA,EAAEiB,EAAKjB,IACjB,KAAGD,EAAEmB,EAAG,GAAKnB,EAAEmB,GAAIT,EAAEjB,IAAMQ,EAAEmB,EAAG,GAAKnB,EAAEmB,GAAIV,EAAEhB,IAC7C,IAAI,IAAIhF,EAAE,EAAEA,EAAEgG,EAAEf,MAAMjF,IACrB4G,EAAEnB,IAAIH,EAAEC,EAAEvF,EAAEgG,EAAEzF,IAAI+E,EAAEmB,EAAGlB,EAAEmB,EAAG1G,SAKjC4G,EAAIZ,EAGN,GAAGW,EAAQ,CAET,IAAIE,EAAKD,EAAEd,eACX,IAAQR,EAAE,EAAEA,EAAEsB,EAAE7B,GAAGO,IACjB,IAAQC,EAAE,EAAEA,EAAEqB,EAAE5B,GAAGO,IACjB,IAAQvF,EAAE,EAAEA,EAAE4G,EAAE3B,MAAMjF,IACrB6G,EAAGpB,IAAIH,EAAEC,EAAEvF,EAAE4G,EAAErG,IAAIqG,EAAE7B,GAAKO,EAAI,EAAEC,EAAEvF,IAIvC4G,EAAIC,EAEN,OAAOD,GA8DThF,EAAOkF,WAzDU,SAASC,EAAKC,GAE7B,QAA+B,IAAtB,EAAuCA,GAAoB,EAEpE,IAAIC,EAASC,SAASC,cAAc,UACpCF,EAAOG,MAAQL,EAAIK,MACnBH,EAAOI,OAASN,EAAIM,OACpB,IAAIC,EAAML,EAAOM,WAAW,MAG5B,IACED,EAAIE,UAAUT,EAAK,EAAG,GACtB,MAAOU,GACP,GAAe,2BAAXA,EAAExH,KAEJ,OAAO,EAEP,MAAMwH,EAIV,IACE,IAAIC,EAAWJ,EAAIK,aAAa,EAAG,EAAGV,EAAOG,MAAOH,EAAOI,QAC3D,MAAOI,GACP,GAAc,mBAAXA,EAAExH,KACH,OAAO,EAEP,MAAMwH,EASV,IAJA,IAAIjG,EAAIkG,EAASE,KACbhB,EAAIG,EAAIK,MACRS,EAAId,EAAIM,OACRS,EAAK,GACDpI,EAAE,EAAEA,EAAE8B,EAAEoB,OAAOlD,IACrBoI,EAAGrD,KAAKjD,EAAE9B,GAAG,IAAM,IAErB,IAAI4F,EAAI,IAAIR,EAAI8B,EAAGiB,EAAG,EAAG,GAGzB,GAFAvC,EAAE9B,EAAIsE,EAEHd,EAAmB,CAEpB,IAAIe,EAAK,IAAIjD,EAAI8B,EAAGiB,EAAG,EAAG,GAC1B,IAAQnI,EAAE,EAAEA,EAAEkH,EAAElH,IACd,IAAI,IAAIsE,EAAE,EAAEA,EAAE6D,EAAE7D,IACd+D,EAAGtC,IAAI/F,EAAEsE,EAAE,EAAEsB,EAAE/E,IAAIb,EAAEsE,EAAE,IAG3BsB,EAAIyC,EAGN,OAAOzC,GArGX,CA2GG5D,GACH,SAAUE,GACR,aACA,IAAIkD,EAAMlD,EAAOkD,IAQbkD,EAAY,SAASrD,GACnBA,EAAMA,GAAO,GAGjBQ,KAAK8C,UAAYtD,EAAIuD,QACrB/C,KAAKJ,GAAKJ,EAAII,GACdI,KAAKgD,SAAWxD,EAAIwD,SACpBhD,KAAKiD,MAAQzD,EAAIyD,MACjBjD,KAAKkD,MAAQ1D,EAAI0D,MAGjBlD,KAAKH,QAAuB,IAAXL,EAAIK,GAAqBL,EAAIK,GAAKG,KAAKJ,GACxDI,KAAKmD,YAA+B,IAAf3D,EAAI2D,OAAyB3D,EAAI2D,OAAS,EAC/DnD,KAAKoD,SAAyB,IAAZ5D,EAAI4D,IAAsB5D,EAAI4D,IAAM,EACtDpD,KAAKqD,kBAA2C,IAArB7D,EAAI6D,aAA+B7D,EAAI6D,aAAe,EACjFrD,KAAKsD,kBAA2C,IAArB9D,EAAI8D,aAA+B9D,EAAI8D,aAAe,EAMjFtD,KAAKuD,OAASzG,KAAKa,OAAOqC,KAAKiD,MAAmB,EAAXjD,KAAKoD,IAAUpD,KAAKJ,IAAMI,KAAKmD,OAAS,GAC/EnD,KAAKwD,OAAS1G,KAAKa,OAAOqC,KAAKkD,MAAmB,EAAXlD,KAAKoD,IAAUpD,KAAKH,IAAMG,KAAKmD,OAAS,GAC/EnD,KAAKyD,WAAa,OAGlB,IAAIC,OAAgC,IAAlBlE,EAAImE,UAA4BnE,EAAImE,UAAY,EAClE3D,KAAK+C,QAAU,GACf,IAAI,IAAIxI,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IAAOyF,KAAK+C,QAAQzD,KAAK,IAAIK,EAAIK,KAAKJ,GAAII,KAAKH,GAAIG,KAAKgD,WACrFhD,KAAK4D,OAAS,IAAIjE,EAAI,EAAG,EAAGK,KAAK8C,UAAWY,IAE9Cb,EAAU1G,UAAY,CACpB0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EAGd,IADA,IAAImD,EAAI,IAAIrE,EAAIK,KAAKuD,OAAQvD,KAAKwD,OAAQxD,KAAK8C,UAAW,GAClDjI,EAAE,EAAEA,EAAEmF,KAAK8C,UAAUjI,IAI3B,IAHA,IAAIoJ,EAAIjE,KAAK+C,QAAQlI,GACjBsF,GAAKH,KAAKoD,IACVhD,GAAKJ,KAAKoD,IACNc,EAAG,EAAGA,EAAGlE,KAAKuD,OAAQpD,GAAGH,KAAKmD,OAAOe,IAAM,CACjD9D,GAAKJ,KAAKoD,IACV,IAAI,IAAIe,EAAG,EAAGA,EAAGnE,KAAKwD,OAAQpD,GAAGJ,KAAKmD,OAAOgB,IAAM,CAKjD,IADA,IAAI/G,EAAI,EACAgH,EAAG,EAAEA,EAAGH,EAAErE,GAAGwE,IACnB,IAAI,IAAIC,EAAG,EAAEA,EAAGJ,EAAEpE,GAAGwE,IACnB,IAAI,IAAIC,EAAG,EAAEA,EAAGL,EAAEnE,MAAMwE,IAAM,CAC5B,IAAIC,EAAKnE,EAAEiE,EACPG,EAAKrE,EAAEiE,EACRG,GAAI,GAAKA,EAAG1D,EAAEhB,IAAM2E,GAAI,GAAKA,EAAG3D,EAAEjB,KAGnCxC,GAAK6G,EAAE5F,GAAI4F,EAAErE,GAAKyE,EAAID,GAAIH,EAAEnE,MAAMwE,GAAMzD,EAAExC,GAAIwC,EAAEjB,GAAK2E,EAAIC,GAAI3D,EAAEf,MAAMwE,IAK7ElH,GAAK4C,KAAK4D,OAAOvF,EAAExD,GACnBmJ,EAAE1D,IAAI4D,EAAIC,EAAItJ,EAAGuC,IAKvB,OADA4C,KAAKyE,QAAUT,EACRhE,KAAKyE,SAEdC,SAAU,WAGR,IAAI7D,EAAIb,KAAK+D,OACblD,EAAEZ,GAAKxD,EAAOsB,MAAM8C,EAAExC,EAAEZ,QACxB,IAAI,IAAI5C,EAAE,EAAEA,EAAEmF,KAAK8C,UAAUjI,IAI3B,IAHA,IAAIoJ,EAAIjE,KAAK+C,QAAQlI,GACjBsF,GAAKH,KAAKoD,IACVhD,GAAKJ,KAAKoD,IACNc,EAAG,EAAGA,EAAGlE,KAAKuD,OAAQpD,GAAGH,KAAKmD,OAAOe,IAAM,CACjD9D,GAAKJ,KAAKoD,IACV,IAAI,IAAIe,EAAG,EAAGA,EAAGnE,KAAKwD,OAAQpD,GAAGJ,KAAKmD,OAAOgB,IAAM,CAIjD,IADA,IAAIQ,EAAa3E,KAAKyE,QAAQjE,SAAS0D,EAAGC,EAAGtJ,GACrCuJ,EAAG,EAAEA,EAAGH,EAAErE,GAAGwE,IACnB,IAAI,IAAIC,EAAG,EAAEA,EAAGJ,EAAEpE,GAAGwE,IACnB,IAAI,IAAIC,EAAG,EAAEA,EAAGL,EAAEnE,MAAMwE,IAAM,CAC5B,IAAIC,EAAKnE,EAAEiE,EACPG,EAAKrE,EAAEiE,EACX,GAAGG,GAAI,GAAKA,EAAG1D,EAAEhB,IAAM2E,GAAI,GAAKA,EAAG3D,EAAEjB,GAAI,CAMvC,IAAIgF,GAAQ/D,EAAEjB,GAAK2E,EAAIC,GAAI3D,EAAEf,MAAMwE,EAC/BO,GAAQZ,EAAErE,GAAKyE,EAAID,GAAIH,EAAEnE,MAAMwE,EACnCL,EAAEhE,GAAG4E,IAAQhE,EAAExC,EAAEuG,GAAKD,EACtB9D,EAAEZ,GAAG2E,IAAQX,EAAE5F,EAAEwG,GAAKF,GAK9B3E,KAAK4D,OAAO3D,GAAGpF,IAAM8J,KAK7BG,kBAAmB,WAEjB,IADA,IAAIC,EAAW,GACPxK,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IAC3BwK,EAASzF,KAAK,CAAC0F,OAAQhF,KAAK+C,QAAQxI,GAAG8D,EAAG4G,MAAOjF,KAAK+C,QAAQxI,GAAG0F,GAAIqD,aAActD,KAAKsD,aAAcD,aAAcrD,KAAKqD,eAG3H,OADA0B,EAASzF,KAAK,CAAC0F,OAAQhF,KAAK4D,OAAOvF,EAAG4G,MAAOjF,KAAK4D,OAAO3D,GAAIoD,aAAc,EAAKC,aAAc,IACvFyB,GAET9D,OAAQ,WACN,IAAIC,EAAO,GACXA,EAAKtB,GAAKI,KAAKJ,GACfsB,EAAKrB,GAAKG,KAAKH,GACfqB,EAAKiC,OAASnD,KAAKmD,OACnBjC,EAAK8B,SAAWhD,KAAKgD,SACrB9B,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WACvBvC,EAAKmC,aAAerD,KAAKqD,aACzBnC,EAAKoC,aAAetD,KAAKsD,aACzBpC,EAAKkC,IAAMpD,KAAKoD,IAChBlC,EAAK6B,QAAU,GACf,IAAI,IAAIxI,EAAE,EAAEA,EAAEyF,KAAK+C,QAAQtF,OAAOlD,IAChC2G,EAAK6B,QAAQzD,KAAKU,KAAK+C,QAAQxI,GAAG0G,UAGpC,OADAC,EAAK0C,OAAS5D,KAAK4D,OAAO3C,SACnBC,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,WACvBzD,KAAKJ,GAAKsB,EAAKtB,GACfI,KAAKH,GAAKqB,EAAKrB,GACfG,KAAKmD,OAASjC,EAAKiC,OACnBnD,KAAKgD,SAAW9B,EAAK8B,SACrBhD,KAAK+C,QAAU,GACf/C,KAAKqD,kBAA4C,IAAtBnC,EAAKmC,aAA+BnC,EAAKmC,aAAe,EACnFrD,KAAKsD,kBAA4C,IAAtBpC,EAAKoC,aAA+BpC,EAAKoC,aAAe,EACnFtD,KAAKoD,SAA0B,IAAblC,EAAKkC,IAAsBlC,EAAKkC,IAAM,EACxD,IAAI,IAAI7I,EAAE,EAAEA,EAAE2G,EAAK6B,QAAQtF,OAAOlD,IAAK,CACrC,IAAIyC,EAAI,IAAI2C,EAAI,EAAE,EAAE,EAAE,GACtB3C,EAAEmE,SAASD,EAAK6B,QAAQxI,IACxByF,KAAK+C,QAAQzD,KAAKtC,GAEpBgD,KAAK4D,OAAS,IAAIjE,EAAI,EAAE,EAAE,EAAE,GAC5BK,KAAK4D,OAAOzC,SAASD,EAAK0C,UAI9B,IAAIsB,EAAiB,SAAS1F,GACxBA,EAAMA,GAAO,GAIjBQ,KAAK8C,eAAuC,IAApBtD,EAAI2F,YAA8B3F,EAAI2F,YAAc3F,EAAIuD,QAGhF/C,KAAKqD,kBAA2C,IAArB7D,EAAI6D,aAA+B7D,EAAI6D,aAAe,EACjFrD,KAAKsD,kBAA2C,IAArB9D,EAAI8D,aAA+B9D,EAAI8D,aAAe,EAGjFtD,KAAKoF,WAAa5F,EAAIyD,MAAQzD,EAAI0D,MAAQ1D,EAAIwD,SAC9ChD,KAAKuD,OAAS,EACdvD,KAAKwD,OAAS,EACdxD,KAAKyD,WAAa,KAGlB,IAAIC,OAAgC,IAAlBlE,EAAImE,UAA4BnE,EAAImE,UAAY,EAClE3D,KAAK+C,QAAU,GACf,IAAI,IAAIxI,EAAE,EAAEA,EAAEyF,KAAK8C,UAAWvI,IAAOyF,KAAK+C,QAAQzD,KAAK,IAAIK,EAAI,EAAG,EAAGK,KAAKoF,aAC1EpF,KAAK4D,OAAS,IAAIjE,EAAI,EAAG,EAAGK,KAAK8C,UAAWY,IAG9CwB,EAAe/I,UAAY,CACzB0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EAGd,IAFA,IAAImD,EAAI,IAAIrE,EAAI,EAAG,EAAGK,KAAK8C,UAAW,GAClCuC,EAAKxE,EAAExC,EACH9D,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IAAK,CAGhC,IAFA,IAAI6C,EAAI,EACJkI,EAAKtF,KAAK+C,QAAQxI,GAAG8D,EACjBxD,EAAE,EAAEA,EAAEmF,KAAKoF,WAAWvK,IAC5BuC,GAAKiI,EAAGxK,GAAKyK,EAAGzK,GAElBuC,GAAK4C,KAAK4D,OAAOvF,EAAE9D,GACnByJ,EAAE3F,EAAE9D,GAAK6C,EAGX,OADA4C,KAAKyE,QAAUT,EACRhE,KAAKyE,SAEdC,SAAU,WACR,IAAI7D,EAAIb,KAAK+D,OACblD,EAAEZ,GAAKxD,EAAOsB,MAAM8C,EAAExC,EAAEZ,QAGxB,IAAI,IAAIlD,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IAAK,CAGhC,IAFA,IAAIgL,EAAMvF,KAAK+C,QAAQxI,GACnBoK,EAAa3E,KAAKyE,QAAQxE,GAAG1F,GACzBM,EAAE,EAAEA,EAAEmF,KAAKoF,WAAWvK,IAC5BgG,EAAEZ,GAAGpF,IAAM0K,EAAIlH,EAAExD,GAAG8J,EACpBY,EAAItF,GAAGpF,IAAMgG,EAAExC,EAAExD,GAAG8J,EAEtB3E,KAAK4D,OAAO3D,GAAG1F,IAAMoK,IAGzBG,kBAAmB,WAEjB,IADA,IAAIC,EAAW,GACPxK,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IAC3BwK,EAASzF,KAAK,CAAC0F,OAAQhF,KAAK+C,QAAQxI,GAAG8D,EAAG4G,MAAOjF,KAAK+C,QAAQxI,GAAG0F,GAAIoD,aAAcrD,KAAKqD,aAAcC,aAActD,KAAKsD,eAG3H,OADAyB,EAASzF,KAAK,CAAC0F,OAAQhF,KAAK4D,OAAOvF,EAAG4G,MAAOjF,KAAK4D,OAAO3D,GAAIoD,aAAc,EAAKC,aAAc,IACvFyB,GAET9D,OAAQ,WACN,IAAIC,EAAO,GACXA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WACvBvC,EAAKkE,WAAapF,KAAKoF,WACvBlE,EAAKmC,aAAerD,KAAKqD,aACzBnC,EAAKoC,aAAetD,KAAKsD,aACzBpC,EAAK6B,QAAU,GACf,IAAI,IAAIxI,EAAE,EAAEA,EAAEyF,KAAK+C,QAAQtF,OAAOlD,IAChC2G,EAAK6B,QAAQzD,KAAKU,KAAK+C,QAAQxI,GAAG0G,UAGpC,OADAC,EAAK0C,OAAS5D,KAAK4D,OAAO3C,SACnBC,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,WACvBzD,KAAKoF,WAAalE,EAAKkE,WACvBpF,KAAKqD,kBAA4C,IAAtBnC,EAAKmC,aAA+BnC,EAAKmC,aAAe,EACnFrD,KAAKsD,kBAA4C,IAAtBpC,EAAKoC,aAA+BpC,EAAKoC,aAAe,EACnFtD,KAAK+C,QAAU,GACf,IAAI,IAAIxI,EAAE,EAAEA,EAAE2G,EAAK6B,QAAQtF,OAAOlD,IAAK,CACrC,IAAIyC,EAAI,IAAI2C,EAAI,EAAE,EAAE,EAAE,GACtB3C,EAAEmE,SAASD,EAAK6B,QAAQxI,IACxByF,KAAK+C,QAAQzD,KAAKtC,GAEpBgD,KAAK4D,OAAS,IAAIjE,EAAI,EAAE,EAAE,EAAE,GAC5BK,KAAK4D,OAAOzC,SAASD,EAAK0C,UAI9BnH,EAAOoG,UAAYA,EACnBpG,EAAOyI,eAAiBA,EA5Q1B,CA8QG3I,GACH,SAAUE,GACR,aACA,IAAIkD,EAAMlD,EAAOkD,IAEb6F,EAAY,SAAShG,GAEnBA,EAAMA,GAAO,GAGjBQ,KAAKJ,GAAKJ,EAAII,GACdI,KAAKgD,SAAWxD,EAAIwD,SACpBhD,KAAKiD,MAAQzD,EAAIyD,MACjBjD,KAAKkD,MAAQ1D,EAAI0D,MAGjBlD,KAAKH,QAAuB,IAAXL,EAAIK,GAAqBL,EAAIK,GAAKG,KAAKJ,GACxDI,KAAKmD,YAA+B,IAAf3D,EAAI2D,OAAyB3D,EAAI2D,OAAS,EAC/DnD,KAAKoD,SAAyB,IAAZ5D,EAAI4D,IAAsB5D,EAAI4D,IAAM,EAGtDpD,KAAK8C,UAAY9C,KAAKgD,SACtBhD,KAAKuD,OAASzG,KAAKa,OAAOqC,KAAKiD,MAAmB,EAAXjD,KAAKoD,IAAUpD,KAAKJ,IAAMI,KAAKmD,OAAS,GAC/EnD,KAAKwD,OAAS1G,KAAKa,OAAOqC,KAAKkD,MAAmB,EAAXlD,KAAKoD,IAAUpD,KAAKH,IAAMG,KAAKmD,OAAS,GAC/EnD,KAAKyD,WAAa,OAElBzD,KAAKyF,QAAUhJ,EAAOsB,MAAMiC,KAAKuD,OAAOvD,KAAKwD,OAAOxD,KAAK8C,WACzD9C,KAAK0F,QAAUjJ,EAAOsB,MAAMiC,KAAKuD,OAAOvD,KAAKwD,OAAOxD,KAAK8C,YAG3D0C,EAAUrJ,UAAY,CACpB0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EAKd,IAHA,IAAImD,EAAI,IAAIrE,EAAIK,KAAKuD,OAAQvD,KAAKwD,OAAQxD,KAAK8C,UAAW,GAEtD9G,EAAE,EACEnB,EAAE,EAAEA,EAAEmF,KAAK8C,UAAUjI,IAG3B,IAFA,IAAIsF,GAAKH,KAAKoD,IACVhD,GAAKJ,KAAKoD,IACNc,EAAG,EAAGA,EAAGlE,KAAKuD,OAAQpD,GAAGH,KAAKmD,OAAOe,IAAM,CACjD9D,GAAKJ,KAAKoD,IACV,IAAI,IAAIe,EAAG,EAAGA,EAAGnE,KAAKwD,OAAQpD,GAAGJ,KAAKmD,OAAOgB,IAAM,CAKjD,IAFA,IAAI/G,GAAK,MACLuI,GAAM,EAAEC,GAAM,EACVxB,EAAG,EAAEA,EAAGpE,KAAKJ,GAAGwE,IACtB,IAAI,IAAIC,EAAG,EAAEA,EAAGrE,KAAKH,GAAGwE,IAAM,CAC5B,IAAIE,EAAKnE,EAAEiE,EACPG,EAAKrE,EAAEiE,EACX,GAAGG,GAAI,GAAKA,EAAG1D,EAAEhB,IAAM2E,GAAI,GAAKA,EAAG3D,EAAEjB,GAAI,CACvC,IAAI5C,EAAI6D,EAAEzF,IAAIoJ,EAAID,EAAI1J,GAInBmC,EAAII,IAAKA,EAAIJ,EAAG2I,EAAKnB,EAAIoB,EAAKrB,IAIvCvE,KAAKyF,QAAQzJ,GAAK2J,EAClB3F,KAAK0F,QAAQ1J,GAAK4J,EAClB5J,IACAgI,EAAE1D,IAAI4D,EAAIC,EAAItJ,EAAGuC,IAKvB,OADA4C,KAAKyE,QAAUT,EACRhE,KAAKyE,SAEdC,SAAU,WAGR,IAAI7D,EAAIb,KAAK+D,OACblD,EAAEZ,GAAKxD,EAAOsB,MAAM8C,EAAExC,EAAEZ,QAChBuC,KAAKyE,QAGb,IAHA,IAEIzI,EAAI,EACAnB,EAAE,EAAEA,EAAEmF,KAAK8C,UAAUjI,IAC3B,CAASmF,KAAKoD,IACLpD,KAAKoD,IACd,IAFA,IAEQc,EAAG,EAAGA,EAAGlE,KAAKuD,OAAWvD,KAAKmD,OAAOe,IAAM,EAC5ClE,KAAKoD,IACV,IAAI,IAAIe,EAAG,EAAGA,EAAGnE,KAAKwD,OAAWxD,KAAKmD,OAAOgB,IAAM,CAEjD,IAAIQ,EAAa3E,KAAKyE,QAAQjE,SAAS0D,EAAGC,EAAGtJ,GAC7CgG,EAAEH,SAASV,KAAKyF,QAAQzJ,GAAIgE,KAAK0F,QAAQ1J,GAAInB,EAAG8J,GAChD3I,QAMR8I,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAUX,OATAA,EAAKtB,GAAKI,KAAKJ,GACfsB,EAAKrB,GAAKG,KAAKH,GACfqB,EAAKiC,OAASnD,KAAKmD,OACnBjC,EAAK8B,SAAWhD,KAAKgD,SACrB9B,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WACvBvC,EAAKkC,IAAMpD,KAAKoD,IACTlC,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,WACvBzD,KAAKJ,GAAKsB,EAAKtB,GACfI,KAAKH,GAAKqB,EAAKrB,GACfG,KAAKmD,OAASjC,EAAKiC,OACnBnD,KAAKgD,SAAW9B,EAAK8B,SACrBhD,KAAKoD,SAA0B,IAAblC,EAAKkC,IAAsBlC,EAAKkC,IAAM,EACxDpD,KAAKyF,QAAUhJ,EAAOsB,MAAMiC,KAAKuD,OAAOvD,KAAKwD,OAAOxD,KAAK8C,WACzD9C,KAAK0F,QAAUjJ,EAAOsB,MAAMiC,KAAKuD,OAAOvD,KAAKwD,OAAOxD,KAAK8C,aAI7DrG,EAAO+I,UAAYA,EA3HrB,CA6HGjJ,GAEH,SAAUE,GACR,aACUA,EAAOkD,IAAjB,IAEIkG,EAAa,SAASrG,GACpBA,EAAMA,GAAO,GAGjBQ,KAAKuD,YAA+B,IAAf/D,EAAI+D,OAAyB/D,EAAI+D,OAAS/D,EAAIyD,MACnEjD,KAAKwD,YAA+B,IAAfhE,EAAIgE,OAAyBhE,EAAIgE,OAAShE,EAAI0D,MACnElD,KAAK8C,eAAqC,IAAlBtD,EAAIsD,UAA4BtD,EAAIsD,UAAYtD,EAAIwD,SAC5EhD,KAAKyD,WAAa,SAEpBoC,EAAW1J,UAAY,CACrB0H,QAAS,SAAShD,EAAGiD,GAGnB,OAFA9D,KAAK+D,OAASlD,EACdb,KAAKyE,QAAU5D,EACRb,KAAKyE,SAEdC,SAAU,aACVI,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAKX,OAJAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WAChBvC,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,aAI3BhH,EAAOoJ,WAAaA,EAvCtB,CAwCGtJ,GACH,SAAUE,GACR,aACA,IAAIkD,EAAMlD,EAAOkD,IAWbmG,EAAe,SAAStG,GACtBA,EAAMA,GAAO,GAGjBQ,KAAKoF,WAAa5F,EAAIyD,MAAQzD,EAAI0D,MAAQ1D,EAAIwD,SAC9ChD,KAAK8C,UAAY9C,KAAKoF,WACtBpF,KAAKuD,OAAS,EACdvD,KAAKwD,OAAS,EACdxD,KAAKyD,WAAa,WAGpBqC,EAAa3J,UAAY,CACvB0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EAOd,IALA,IAAImD,EAAI,IAAIrE,EAAI,EAAG,EAAGK,KAAK8C,UAAW,GAGlCiD,EAAKlF,EAAExC,EACP2H,EAAOnF,EAAExC,EAAE,GACP9D,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IACxBwL,EAAGxL,GAAKyL,IAAMA,EAAOD,EAAGxL,IAI7B,IAAI0L,EAAKxJ,EAAOsB,MAAMiC,KAAK8C,WACvBoD,EAAO,EACX,IAAQ3L,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IAAK,CAChC,IAAI+H,EAAIxF,KAAKqJ,IAAIJ,EAAGxL,GAAKyL,GACzBE,GAAQ5D,EACR2D,EAAG1L,GAAK+H,EAIV,IAAQ/H,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IAC3B0L,EAAG1L,IAAM2L,EACTlC,EAAE3F,EAAE9D,GAAK0L,EAAG1L,GAKd,OAFAyF,KAAKiG,GAAKA,EACVjG,KAAKyE,QAAUT,EACRhE,KAAKyE,SAEdC,SAAU,SAAStE,GAGjB,IAAID,EAAIH,KAAK+D,OACb5D,EAAEF,GAAKxD,EAAOsB,MAAMoC,EAAE9B,EAAEZ,QAExB,IAAI,IAAIlD,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IAAK,CAChC,IACI6L,KADY7L,IAAM6F,EAAI,EAAM,GACRJ,KAAKiG,GAAG1L,IAChC4F,EAAEF,GAAG1F,GAAK6L,EAIZ,OAAQtJ,KAAKI,IAAI8C,KAAKiG,GAAG7F,KAE3B0E,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAMX,OALAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WACvBvC,EAAKkE,WAAapF,KAAKoF,WAChBlE,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,WACvBzD,KAAKoF,WAAalE,EAAKkE,aAO3B,IAAIiB,EAAkB,SAAS7G,GACzBA,EAAMA,GAAO,GAGjBQ,KAAKoF,WAAa5F,EAAIyD,MAAQzD,EAAI0D,MAAQ1D,EAAIwD,SAC9ChD,KAAK8C,UAAY9C,KAAKoF,WACtBpF,KAAKuD,OAAS,EACdvD,KAAKwD,OAAS,EACdxD,KAAKyD,WAAa,cAGpB4C,EAAgBlK,UAAY,CAC1B0H,QAAS,SAAShD,EAAGiD,GAGnB,OAFA9D,KAAK+D,OAASlD,EACdb,KAAKyE,QAAU5D,EACRA,GAGT6D,SAAU,SAAStE,GAGjB,IAAID,EAAIH,KAAK+D,OACb5D,EAAEF,GAAKxD,EAAOsB,MAAMoC,EAAE9B,EAAEZ,QACxB,IAAI6I,EAAO,EACX,GAAGlG,aAAalC,OAASkC,aAAajC,aACpC,IAAI,IAAI5D,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,IAAK,CAChC,IAAIgH,EAAKpB,EAAE9B,EAAE9D,GAAK6F,EAAE7F,GACpB4F,EAAEF,GAAG1F,GAAKgH,EACV+E,GAAQ,EAAE/E,EAAGA,MAEV,CAGDhH,EAAI6F,EAAEmG,IAAV,IACIC,EAAKpG,EAAEqG,IACPlF,EAAKpB,EAAE9B,EAAE9D,GAAKiM,EAClBrG,EAAEF,GAAG1F,GAAKgH,EACV+E,GAAQ,EAAE/E,EAAGA,EAEf,OAAO+E,GAETxB,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAMX,OALAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WACvBvC,EAAKkE,WAAapF,KAAKoF,WAChBlE,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,WACvBzD,KAAKoF,WAAalE,EAAKkE,aAI3B,IAAIsB,EAAW,SAASlH,GAClBA,EAAMA,GAAO,GAGjBQ,KAAKoF,WAAa5F,EAAIyD,MAAQzD,EAAI0D,MAAQ1D,EAAIwD,SAC9ChD,KAAK8C,UAAY9C,KAAKoF,WACtBpF,KAAKuD,OAAS,EACdvD,KAAKwD,OAAS,EACdxD,KAAKyD,WAAa,OAGpBiD,EAASvK,UAAY,CACnB0H,QAAS,SAAShD,EAAGiD,GAGnB,OAFA9D,KAAK+D,OAASlD,EACdb,KAAKyE,QAAU5D,EACRA,GAET6D,SAAU,SAAStE,GAGjB,IAAID,EAAIH,KAAK+D,OACb5D,EAAEF,GAAKxD,EAAOsB,MAAMoC,EAAE9B,EAAEZ,QAKxB,IAHA,IAAIkJ,EAASxG,EAAE9B,EAAE+B,GAEbkG,EAAO,EACH/L,EAAE,EAAEA,EAAEyF,KAAK8C,UAAUvI,KACvBoM,EAASxG,EAAE9B,EAAE9D,GAHN,EAGoB,IAM7B4F,EAAEF,GAAG1F,IAAM,EACX4F,EAAEF,GAAGG,IAAM,EACXkG,IAASK,EAASxG,EAAE9B,EAAE9D,GAXb,GAeb,OAAO+L,GAETxB,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAMX,OALAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WACvBvC,EAAKkE,WAAapF,KAAKoF,WAChBlE,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,WACvBzD,KAAKoF,WAAalE,EAAKkE,aAI3B3I,EAAO4J,gBAAkBA,EACzB5J,EAAOqJ,aAAeA,EACtBrJ,EAAOiK,SAAWA,EA7NpB,CA+NGnK,GAEH,SAAUE,GACR,aACA,IAAIkD,EAAMlD,EAAOkD,IAKbiH,EAAY,SAASpH,GACnBA,EAAMA,GAAO,GAGjBQ,KAAKuD,OAAS/D,EAAIyD,MAClBjD,KAAKwD,OAAShE,EAAI0D,MAClBlD,KAAK8C,UAAYtD,EAAIwD,SACrBhD,KAAKyD,WAAa,QAEpBmD,EAAUzK,UAAY,CACpB0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EAId,IAHA,IAAIgG,EAAKhG,EAAED,QACPkG,EAAIjG,EAAExC,EAAEZ,OACRsJ,EAAMF,EAAGxI,EACL9D,EAAE,EAAEA,EAAEuM,EAAEvM,IACXwM,EAAIxM,GAAK,IAAGwM,EAAIxM,GAAK,GAG1B,OADAyF,KAAKyE,QAAUoC,EACR7G,KAAKyE,SAEdC,SAAU,WACR,IAAI7D,EAAIb,KAAK+D,OACT8C,EAAK7G,KAAKyE,QACVqC,EAAIjG,EAAExC,EAAEZ,OACZoD,EAAEZ,GAAKxD,EAAOsB,MAAM+I,GACpB,IAAI,IAAIvM,EAAE,EAAEA,EAAEuM,EAAEvM,IACXsM,EAAGxI,EAAE9D,IAAM,EAAGsG,EAAEZ,GAAG1F,GAAK,EACtBsG,EAAEZ,GAAG1F,GAAKsM,EAAG5G,GAAG1F,IAGzBuK,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAKX,OAJAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WAChBvC,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,aAO3B,IAAIuD,EAAe,SAASxH,GACtBA,EAAMA,GAAO,GAGjBQ,KAAKuD,OAAS/D,EAAIyD,MAClBjD,KAAKwD,OAAShE,EAAI0D,MAClBlD,KAAK8C,UAAYtD,EAAIwD,SACrBhD,KAAKyD,WAAa,WAEpBuD,EAAa7K,UAAY,CACvB0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EAKd,IAJA,IAAIgG,EAAKhG,EAAEF,eACPmG,EAAIjG,EAAExC,EAAEZ,OACRsJ,EAAMF,EAAGxI,EACTgH,EAAKxE,EAAExC,EACH9D,EAAE,EAAEA,EAAEuM,EAAEvM,IACdwM,EAAIxM,GAAK,GAAK,EAAIuC,KAAKqJ,KAAKd,EAAG9K,KAGjC,OADAyF,KAAKyE,QAAUoC,EACR7G,KAAKyE,SAEdC,SAAU,WACR,IAAI7D,EAAIb,KAAK+D,OACT8C,EAAK7G,KAAKyE,QACVqC,EAAIjG,EAAExC,EAAEZ,OACZoD,EAAEZ,GAAKxD,EAAOsB,MAAM+I,GACpB,IAAI,IAAIvM,EAAE,EAAEA,EAAEuM,EAAEvM,IAAK,CACnB,IAAI0M,EAAOJ,EAAGxI,EAAE9D,GAChBsG,EAAEZ,GAAG1F,GAAM0M,GAAQ,EAAMA,GAAQJ,EAAG5G,GAAG1F,KAG3CuK,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAKX,OAJAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WAChBvC,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,aAQ3B,IAAIyD,EAAc,SAAS1H,GACrBA,EAAMA,GAAO,GAGjBQ,KAAKmH,gBAAuC,IAAnB3H,EAAI2H,WAA6B3H,EAAI2H,WAAa,EAG3EnH,KAAKuD,OAAS/D,EAAIyD,MAClBjD,KAAKwD,OAAShE,EAAI0D,MAClBlD,KAAK8C,UAAYhG,KAAKa,MAAM6B,EAAIwD,SAAWhD,KAAKmH,YAChDnH,KAAKyD,WAAa,SAElBzD,KAAKoH,SAAW3K,EAAOsB,MAAMiC,KAAKuD,OAAOvD,KAAKwD,OAAOxD,KAAK8C,YAE5DoE,EAAY/K,UAAY,CACtB0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EACd,IAAIiG,EAAI9G,KAAK8C,UACT+D,EAAK,IAAIlH,EAAIK,KAAKuD,OAAQvD,KAAKwD,OAAQxD,KAAK8C,UAAW,GAK3D,GAAmB,IAAhB9C,KAAKuD,QAAgC,IAAhBvD,KAAKwD,OAC3B,IAAI,IAAIjJ,EAAE,EAAEA,EAAEuM,EAAEvM,IAAK,CAInB,IAHA,IAAI8F,EAAK9F,EAAIyF,KAAKmH,WACd/J,EAAIyD,EAAExC,EAAEgC,GACRgH,EAAK,EACDxI,EAAE,EAAEA,EAAEmB,KAAKmH,WAAWtI,IAAK,EAC7ByI,EAAKzG,EAAExC,EAAEgC,EAAGxB,IACRzB,IACNA,EAAIkK,EACJD,EAAKxI,GAGTgI,EAAGxI,EAAE9D,GAAK6C,EACV4C,KAAKoH,SAAS7M,GAAK8F,EAAKgH,OAI1B,IADA,IAAIrL,EAAE,EACEmE,EAAE,EAAEA,EAAEU,EAAEjB,GAAGO,IACjB,IAAI,IAAIC,EAAE,EAAEA,EAAES,EAAEhB,GAAGO,IACjB,IAAQ7F,EAAE,EAAEA,EAAEuM,EAAEvM,IAAK,CAInB,IAHI8F,EAAK9F,EAAIyF,KAAKmH,WACd/J,EAAIyD,EAAEzF,IAAI+E,EAAGC,EAAGC,GAChBgH,EAAK,EACDxI,EAAE,EAAEA,EAAEmB,KAAKmH,WAAWtI,IAAK,CACjC,IAAIyI,KAAKzG,EAAEzF,IAAI+E,EAAGC,EAAGC,EAAGxB,IAChBzB,IACNA,EAAIkK,EACJD,EAAKxI,GAGTgI,EAAGvG,IAAIH,EAAEC,EAAE7F,EAAE6C,GACb4C,KAAKoH,SAASpL,GAAKqE,EAAKgH,EACxBrL,IAOR,OADAgE,KAAKyE,QAAUoC,EACR7G,KAAKyE,SAEdC,SAAU,WACR,IAAI7D,EAAIb,KAAK+D,OACT8C,EAAK7G,KAAKyE,QACVqC,EAAI9G,KAAK8C,UAIb,GAHAjC,EAAEZ,GAAKxD,EAAOsB,MAAM8C,EAAExC,EAAEZ,QAGL,IAAhBuC,KAAKuD,QAAgC,IAAhBvD,KAAKwD,OAC3B,IAAI,IAAIjJ,EAAE,EAAEA,EAAEuM,EAAEvM,IAAK,CACnB,IAAIoK,EAAakC,EAAG5G,GAAG1F,GACvBsG,EAAEZ,GAAGD,KAAKoH,SAAS7M,IAAMoK,OAK3B,IADA,IAAI3I,EAAE,EACEmE,EAAE,EAAEA,EAAE0G,EAAGjH,GAAGO,IAClB,IAAI,IAAIC,EAAE,EAAEA,EAAEyG,EAAGhH,GAAGO,IAClB,IAAQ7F,EAAE,EAAEA,EAAEuM,EAAEvM,IAAK,CACfoK,EAAakC,EAAGrG,SAASL,EAAEC,EAAE7F,GACjCsG,EAAEJ,SAASN,EAAEC,EAAEJ,KAAKoH,SAASpL,GAAG2I,GAChC3I,MAMV8I,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAMX,OALAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WACvBvC,EAAKiG,WAAanH,KAAKmH,WAChBjG,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,WACvBzD,KAAKmH,WAAajG,EAAKiG,WACvBnH,KAAKoH,SAAW3K,EAAOsB,MAAMiC,KAAKmH,cAYtC,IAAII,EAAY,SAAS/H,GACnBA,EAAMA,GAAO,GAGjBQ,KAAKuD,OAAS/D,EAAIyD,MAClBjD,KAAKwD,OAAShE,EAAI0D,MAClBlD,KAAK8C,UAAYtD,EAAIwD,SACrBhD,KAAKyD,WAAa,QAEpB8D,EAAUpL,UAAY,CACpB0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EAGd,IAFA,IAnBUV,EACRC,EAkBEyG,EAAKhG,EAAEF,eACPmG,EAAIjG,EAAExC,EAAEZ,OACJlD,EAAE,EAAEA,EAAEuM,EAAEvM,IACdsM,EAAGxI,EAAE9D,IAtBG4F,EAsBOU,EAAExC,EAAE9D,GArBnB6F,aAAItD,KAAKqJ,IAAI,EAAIhG,IACT,IAAMC,EAAI,IAuBpB,OADAJ,KAAKyE,QAAUoC,EACR7G,KAAKyE,SAEdC,SAAU,WACR,IAAI7D,EAAIb,KAAK+D,OACT8C,EAAK7G,KAAKyE,QACVqC,EAAIjG,EAAExC,EAAEZ,OACZoD,EAAEZ,GAAKxD,EAAOsB,MAAM+I,GACpB,IAAI,IAAIvM,EAAE,EAAEA,EAAEuM,EAAEvM,IAAK,CACnB,IAAI0M,EAAOJ,EAAGxI,EAAE9D,GAChBsG,EAAEZ,GAAG1F,IAAM,EAAM0M,EAAOA,GAAQJ,EAAG5G,GAAG1F,KAG1CuK,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAKX,OAJAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WAChBvC,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,aAI3BhH,EAAO8K,UAAYA,EACnB9K,EAAOyK,YAAcA,EACrBzK,EAAOmK,UAAYA,EACnBnK,EAAOuK,aAAeA,EA/RxB,CAiSGzK,GAEH,SAAUE,GACR,aACUA,EAAOkD,IAAjB,IAQI6H,EAAe,SAAShI,GACtBA,EAAMA,GAAO,GAGjBQ,KAAKuD,OAAS/D,EAAIyD,MAClBjD,KAAKwD,OAAShE,EAAI0D,MAClBlD,KAAK8C,UAAYtD,EAAIwD,SACrBhD,KAAKyD,WAAa,UAClBzD,KAAKyH,eAAqC,IAAlBjI,EAAIiI,UAA4BjI,EAAIiI,UAAY,GACxEzH,KAAK0H,QAAUjL,EAAOsB,MAAMiC,KAAKuD,OAAOvD,KAAKwD,OAAOxD,KAAK8C,YAE3D0E,EAAarL,UAAY,CACvB0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,OACW,IAAhB,IAA+BiD,GAAc,GACtD,IAAI+C,EAAKhG,EAAED,QACPkG,EAAIjG,EAAExC,EAAEZ,OACZ,GAAGqG,EAED,IAAI,IAAIvJ,EAAE,EAAEA,EAAEuM,EAAEvM,IACXuC,KAAKC,SAASiD,KAAKyH,WAAaZ,EAAGxI,EAAE9D,GAAG,EAAGyF,KAAK0H,QAAQnN,IAAK,GAC1DyF,KAAK0H,QAAQnN,IAAK,OAI1B,IAAQA,EAAE,EAAEA,EAAEuM,EAAEvM,IAAOsM,EAAGxI,EAAE9D,IAAIyF,KAAKyH,UAGvC,OADAzH,KAAKyE,QAAUoC,EACR7G,KAAKyE,SAEdC,SAAU,WACR,IAAI7D,EAAIb,KAAK+D,OACTY,EAAa3E,KAAKyE,QAClBqC,EAAIjG,EAAExC,EAAEZ,OACZoD,EAAEZ,GAAKxD,EAAOsB,MAAM+I,GACpB,IAAI,IAAIvM,EAAE,EAAEA,EAAEuM,EAAEvM,IACTyF,KAAK0H,QAAQnN,KAChBsG,EAAEZ,GAAG1F,GAAKoK,EAAW1E,GAAG1F,KAI9BuK,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAMX,OALAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WACvBvC,EAAKuG,UAAYzH,KAAKyH,UACfvG,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,WACvBzD,KAAKyH,UAAYvG,EAAKuG,YAK1BhL,EAAO+K,aAAeA,EAzExB,CA0EGjL,GACH,SAAUE,GACR,aACUA,EAAOkD,IAAjB,IAKIgI,EAAkC,SAASnI,GACzCA,EAAMA,GAAO,GAGjBQ,KAAKZ,EAAII,EAAIJ,EACbY,KAAKhE,EAAIwD,EAAIxD,EACbgE,KAAK4H,MAAQpI,EAAIoI,MACjB5H,KAAK6H,KAAOrI,EAAIqI,KAGhB7H,KAAKuD,OAAS/D,EAAIyD,MAClBjD,KAAKwD,OAAShE,EAAI0D,MAClBlD,KAAK8C,UAAYtD,EAAIwD,SACrBhD,KAAKyD,WAAa,MAGfzD,KAAKhE,EAAE,GAAM,GAAK8L,QAAQ5K,IAAI,0CAEnCyK,EAAgCxL,UAAY,CAC1C0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EAEd,IAAImD,EAAInD,EAAEF,eACVX,KAAK+H,SAAWlH,EAAEF,eAElB,IADA,IAAIqH,EAAKlL,KAAKa,MAAMqC,KAAKhE,EAAE,GACnBmE,EAAE,EAAEA,EAAEU,EAAEjB,GAAGO,IACjB,IAAI,IAAIC,EAAE,EAAEA,EAAES,EAAEhB,GAAGO,IACjB,IAAI,IAAI7F,EAAE,EAAEA,EAAEsG,EAAEf,MAAMvF,IAAK,CAMzB,IAJA,IAAI8M,EAAKxG,EAAEzF,IAAI+E,EAAEC,EAAE7F,GAGf0N,EAAM,EACFpJ,EAAE/B,KAAKoL,IAAI,EAAE3N,EAAEyN,GAAInJ,GAAG/B,KAAKqL,IAAI5N,EAAEyN,EAAGnH,EAAEf,MAAM,GAAGjB,IAAK,CAC1D,IAAIuJ,EAAKvH,EAAEzF,IAAI+E,EAAEC,EAAEvB,GACnBoJ,GAAOG,EAAGA,EAEZH,GAAOjI,KAAK4H,MAAQ5H,KAAKhE,EACzBiM,GAAOjI,KAAKZ,EACZY,KAAK+H,SAASzH,IAAIH,EAAEC,EAAE7F,EAAE0N,GACxBA,EAAMnL,KAAKuL,IAAIJ,EAAKjI,KAAK6H,MACzB7D,EAAE1D,IAAIH,EAAEC,EAAE7F,EAAE8M,EAAGY,GAMrB,OADAjI,KAAKyE,QAAUT,EACRhE,KAAKyE,SAEdC,SAAU,WAER,IAAI7D,EAAIb,KAAK+D,OACblD,EAAEZ,GAAKxD,EAAOsB,MAAM8C,EAAExC,EAAEZ,QAChBuC,KAAKyE,QAGb,IAHA,IAEIuD,EAAKlL,KAAKa,MAAMqC,KAAKhE,EAAE,GACnBmE,EAAE,EAAEA,EAAEU,EAAEjB,GAAGO,IACjB,IAAI,IAAIC,EAAE,EAAEA,EAAES,EAAEhB,GAAGO,IACjB,IAAI,IAAI7F,EAAE,EAAEA,EAAEsG,EAAEf,MAAMvF,IAQpB,IANA,IAAIoK,EAAa3E,KAAKyE,QAAQjE,SAASL,EAAEC,EAAE7F,GACvC+N,EAAItI,KAAK+H,SAAS3M,IAAI+E,EAAEC,EAAE7F,GAC1BgO,EAAKzL,KAAKuL,IAAIC,EAAGtI,KAAK6H,MACtBW,EAAMD,EAAGA,EAGL1J,EAAE/B,KAAKoL,IAAI,EAAE3N,EAAEyN,GAAInJ,GAAG/B,KAAKqL,IAAI5N,EAAEyN,EAAGnH,EAAEf,MAAM,GAAGjB,IAAK,CAC1D,IAAI4J,EAAK5H,EAAEzF,IAAI+E,EAAEC,EAAEvB,GACf6J,GAAKD,EAAGzI,KAAK6H,KAAK/K,KAAKuL,IAAIC,EAAEtI,KAAK6H,KAAK,GAAG7H,KAAK4H,MAAM5H,KAAKhE,EAAE,EAAEyM,EAC/D5J,IAAItE,IAAGmO,GAAIH,GACdG,GAAKF,EACLE,GAAK/D,EACL9D,EAAEH,SAASP,EAAEC,EAAEvB,EAAE6J,KAO3B5D,kBAAmB,WAAa,MAAO,IACvC7D,OAAQ,WACN,IAAIC,EAAO,GASX,OARAA,EAAK9B,EAAIY,KAAKZ,EACd8B,EAAKlF,EAAIgE,KAAKhE,EACdkF,EAAK0G,MAAQ5H,KAAK4H,MAClB1G,EAAK2G,KAAO7H,KAAK6H,KACjB3G,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKuC,WAAazD,KAAKyD,WAChBvC,GAETC,SAAU,SAASD,GACjBlB,KAAKZ,EAAI8B,EAAK9B,EACdY,KAAKhE,EAAIkF,EAAKlF,EACdgE,KAAK4H,MAAQ1G,EAAK0G,MAClB5H,KAAK6H,KAAO3G,EAAK2G,KACjB7H,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKyD,WAAavC,EAAKuC,aAK3BhH,EAAOkL,gCAAkCA,EAhH3C,CAiHGpL,GACH,SAAUE,GACR,aACA,IAAIkD,EAAMlD,EAAOkD,IAIbgJ,EAAqB,SAASnJ,GAC5BA,EAAMA,GAAO,GAGjBQ,KAAKuD,OAAS/D,EAAIyD,MAClBjD,KAAKwD,OAAShE,EAAI0D,MAMlBlD,KAAK8C,UAAYtD,EAAIwD,SAAWxD,EAAIwD,SAAWxD,EAAIwD,SACnDhD,KAAKyD,WAAa,iBAGpBkF,EAAmBxM,UAAY,CAC7B0H,QAAS,SAAShD,EAAGiD,GACnB9D,KAAK+D,OAASlD,EAId,IAHA,IAAIiG,EAAI9G,KAAK8C,UACT8F,EAAK/H,EAAEf,MACP+G,EAAK,IAAIlH,EAAIK,KAAKuD,OAAQvD,KAAKwD,OAAQxD,KAAK8C,UAAW,GACnD3C,EAAE,EAAEA,EAAEU,EAAEjB,GAAGO,IACjB,IAAI,IAAIC,EAAE,EAAEA,EAAES,EAAEhB,GAAGO,IACjB,IAAI,IAAI7F,EAAE,EAAEA,EAAEuM,EAAEvM,IACd,GAAGA,EAAEqO,EACH/B,EAAGvG,IAAIH,EAAEC,EAAE7F,EAAEsG,EAAEzF,IAAI+E,EAAEC,EAAE7F,QAClB,CACL,IAAIsO,EAAK/L,KAAKa,OAAOpD,EAAEqO,GAAIA,GACvBE,EAAMvO,EAAEqO,EAAMC,EAAGD,EACrB/B,EAAGvG,IAAIH,EAAEC,EAAE7F,EAAEsG,EAAEzF,IAAI+E,EAAEC,EAAEyI,GAAMhI,EAAEzF,IAAI+E,EAAEC,EAAE0I,IAM/C,OADA9I,KAAKyE,QAAUoC,EACR7G,KAAKyE,SAEdC,SAAU,WACR,IAAI7D,EAAIb,KAAK+D,OACblD,EAAEZ,GAAKxD,EAAOsB,MAAM8C,EAAExC,EAAEZ,QAIxB,IAHA,IAAIoJ,EAAK7G,KAAKyE,QACVqC,EAAI9G,KAAK8C,UACT8F,EAAK/H,EAAEf,MACHK,EAAE,EAAEA,EAAEU,EAAEjB,GAAGO,IACjB,IAAI,IAAIC,EAAE,EAAEA,EAAES,EAAEhB,GAAGO,IACjB,IAAI,IAAI7F,EAAE,EAAEA,EAAEuM,EAAEvM,IAAK,CACnB,IAAIoK,EAAakC,EAAGrG,SAASL,EAAEC,EAAE7F,GACjC,GAAGA,EAAEqO,EACH/H,EAAEH,SAASP,EAAEC,EAAE7F,EAAEoK,OACZ,CACL,IAAIkE,EAAK/L,KAAKa,OAAOpD,EAAEqO,GAAIA,GACvBE,EAAMvO,EAAEqO,EAAMC,EAAGD,EACrB/H,EAAEH,SAASP,EAAEC,EAAEyI,EAAGhI,EAAEzF,IAAI+E,EAAEC,EAAE0I,GAAInE,GAChC9D,EAAEH,SAASP,EAAEC,EAAE0I,EAAGjI,EAAEzF,IAAI+E,EAAEC,EAAEyI,GAAIlE,MAM1CG,kBAAmB,WACjB,MAAO,IAET7D,OAAQ,WACN,IAAIC,EAAO,GAKX,OAJAA,EAAK4B,UAAY9C,KAAK8C,UACtB5B,EAAKqC,OAASvD,KAAKuD,OACnBrC,EAAKsC,OAASxD,KAAKwD,OACnBtC,EAAKuC,WAAazD,KAAKyD,WAChBvC,GAETC,SAAU,SAASD,GACjBlB,KAAK8C,UAAY5B,EAAK4B,UACtB9C,KAAKuD,OAASrC,EAAKqC,OACnBvD,KAAKwD,OAAStC,EAAKsC,OACnBxD,KAAKyD,WAAavC,EAAKuC,aAK3BhH,EAAOkM,mBAAqBA,EArF9B,CAsFGpM,GACH,SAAUE,GACR,aACUA,EAAOkD,IAAjB,IAIIoJ,EAAM,SAASC,GACjBhJ,KAAKiJ,OAAS,IAGhBF,EAAI5M,UAAY,CAGd+M,WAAY,SAASC,GAGhBA,EAAK1L,OAAO,GAAIqK,QAAQ5K,IAAI,0DACX,UAAjBiM,EAAK,GAAGC,MAAmBtB,QAAQ5K,IAAI,+CA0D1CiM,EAvDc,WAEZ,IADA,IAAIE,EAAW,GACP9O,EAAE,EAAEA,EAAE4O,EAAK1L,OAAOlD,IAAK,CAC7B,IAAI+O,EAAMH,EAAK5O,GAkCf,GAhCc,YAAX+O,EAAIF,MAA+B,QAAXE,EAAIF,MAG7BC,EAAS/J,KAAK,CAAC8J,KAAK,KAAMjE,YAAamE,EAAIC,cAG/B,eAAXD,EAAIF,MAGLC,EAAS/J,KAAK,CAAC8J,KAAK,KAAMjE,YAAamE,EAAInE,cAG9B,OAAXmE,EAAIF,MAA0B,SAAXE,EAAIF,WACM,IAAnBE,EAAa,YACzBA,EAAI3F,UAAY,OACa,IAAnB2F,EAAIE,YAAiD,SAAnBF,EAAIE,aAC9CF,EAAI3F,UAAY,UAMK,IAAf2F,EAAIG,QAGTH,EAAIG,QACLJ,EAAS/J,KAAK,CAAC8J,KAAM,kBAIzBC,EAAS/J,KAAKgK,QAEe,IAAnBA,EAAIE,WACZ,GAAoB,SAAjBF,EAAIE,WAAuBH,EAAS/J,KAAK,CAAC8J,KAAK,cAC7C,GAAqB,YAAjBE,EAAIE,WAA0BH,EAAS/J,KAAK,CAAC8J,KAAK,iBACtD,GAAqB,SAAjBE,EAAIE,WAAuBH,EAAS/J,KAAK,CAAC8J,KAAK,cACnD,GAAqB,WAAjBE,EAAIE,WAAuB,CAElC,IAAIE,EAAwB,cAAnBJ,EAAInC,WAA6BmC,EAAInC,WAAa,EAC3DkC,EAAS/J,KAAK,CAAC8J,KAAK,SAAUjC,WAAWuC,SAEpC5B,QAAQ5K,IAAI,gCAAkCoM,EAAIE,iBAE/B,IAAlBF,EAAI7B,WAA0C,YAAb6B,EAAIF,MAC7CC,EAAS/J,KAAK,CAAC8J,KAAK,UAAW3B,UAAW6B,EAAI7B,YAIlD,OAAO4B,EAEFM,GAGP3J,KAAKiJ,OAAS,GACd,IAAI,IAAI1O,EAAE,EAAEA,EAAE4O,EAAK1L,OAAOlD,IAAK,CAC7B,IAAI+O,EAAMH,EAAK5O,GACf,GAAGA,EAAE,EAAG,CACN,IAAIqP,EAAO5J,KAAKiJ,OAAO1O,EAAE,GACzB+O,EAAIrG,MAAQ2G,EAAKrG,OACjB+F,EAAIpG,MAAQ0G,EAAKpG,OACjB8F,EAAItG,SAAW4G,EAAK9G,UAGtB,OAAOwG,EAAIF,MACT,IAAK,KAAMpJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOyI,eAAeoE,IAAO,MAC7D,IAAK,MAAOtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOkL,gCAAgC2B,IAAO,MAC/E,IAAK,UAAWtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAO+K,aAAa8B,IAAO,MAChE,IAAK,QAAStJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOoJ,WAAWyD,IAAO,MAC5D,IAAK,UAAWtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOqJ,aAAawD,IAAO,MAChE,IAAK,aAActJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAO4J,gBAAgBiD,IAAO,MACtE,IAAK,OAAQtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOoG,UAAUyG,IAAO,MAC1D,IAAK,OAAQtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAO+I,UAAU8D,IAAO,MAC1D,IAAK,OAAQtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOmK,UAAU0C,IAAO,MAC1D,IAAK,UAAWtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOuK,aAAasC,IAAO,MAChE,IAAK,OAAQtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAO8K,UAAU+B,IAAO,MAC1D,IAAK,SAAUtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOyK,YAAYoC,IAAO,MAC9D,IAAK,gBAAiBtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOkM,mBAAmBW,IAAO,MAC5E,IAAK,MAAOtJ,KAAKiJ,OAAO3J,KAAK,IAAI7C,EAAOiK,SAAS4C,IAAO,MACxD,QAASxB,QAAQ5K,IAAI,sCAM3B2G,QAAS,SAAShD,EAAGiD,QACM,IAAhB,IAA6BA,GAAc,GAEpD,IADA,IAAI+F,EAAM7J,KAAKiJ,OAAO,GAAGpF,QAAQhD,EAAGiD,GAC5BvJ,EAAE,EAAEA,EAAEyF,KAAKiJ,OAAOxL,OAAOlD,IAC/BsP,EAAM7J,KAAKiJ,OAAO1O,GAAGsJ,QAAQgG,EAAK/F,GAEpC,OAAO+F,GAITnF,SAAU,SAAStE,GAGjB,IAFA,IAAI0G,EAAI9G,KAAKiJ,OAAOxL,OAChB6I,EAAOtG,KAAKiJ,OAAOnC,EAAE,GAAGpC,SAAStE,GAC7B7F,EAAEuM,EAAE,EAAEvM,GAAG,EAAEA,IACjByF,KAAKiJ,OAAO1O,GAAGmK,WAEjB,OAAO4B,GAETxB,kBAAmB,WAGjB,IADA,IAAIC,EAAW,GACPxK,EAAE,EAAEA,EAAEyF,KAAKiJ,OAAOxL,OAAOlD,IAE/B,IADA,IAAIuP,EAAgB9J,KAAKiJ,OAAO1O,GAAGuK,oBAC3BjG,EAAE,EAAEA,EAAEiL,EAAcrM,OAAOoB,IACjCkG,EAASzF,KAAKwK,EAAcjL,IAGhC,OAAOkG,GAETgF,cAAe,WAKb,IAJA,IACI1N,EADI2D,KAAKiJ,OAAOjJ,KAAKiJ,OAAOxL,OAAO,GAC7BgH,QAAQpG,EACdC,EAAOjC,EAAE,GACTmC,EAAO,EACHjE,EAAE,EAAEA,EAAE8B,EAAEoB,OAAOlD,IAClB8B,EAAE9B,GAAK+D,IAAQA,EAAOjC,EAAE9B,GAAIiE,EAAOjE,GAExC,OAAOiE,GAETyC,OAAQ,WAGN,IAFA,IAAIC,EAAO,CACX,OAAc,IACN3G,EAAE,EAAEA,EAAEyF,KAAKiJ,OAAOxL,OAAOlD,IAC/B2G,EAAK+H,OAAO3J,KAAKU,KAAKiJ,OAAO1O,GAAG0G,UAElC,OAAOC,GAETC,SAAU,SAASD,GACjBlB,KAAKiJ,OAAS,GACd,IAAI,IAAI1O,EAAE,EAAEA,EAAE2G,EAAK+H,OAAOxL,OAAOlD,IAAK,CACpC,IAEIyP,EAFAC,EAAK/I,EAAK+H,OAAO1O,GACjBkB,EAAIwO,EAAGxG,WAEJ,UAAJhI,IAAeuO,EAAI,IAAIvN,EAAOoJ,YAC1B,SAAJpK,IAAcuO,EAAI,IAAIvN,EAAOmK,WACzB,YAAJnL,IAAiBuO,EAAI,IAAIvN,EAAOuK,cAC5B,SAAJvL,IAAcuO,EAAI,IAAIvN,EAAO8K,WACzB,YAAJ9L,IAAiBuO,EAAI,IAAIvN,EAAO+K,cAC5B,SAAJ/L,IAAcuO,EAAI,IAAIvN,EAAOoG,WACzB,SAAJpH,IAAcuO,EAAI,IAAIvN,EAAO+I,WACzB,QAAJ/J,IAAauO,EAAI,IAAIvN,EAAOkL,iCACxB,YAAJlM,IAAiBuO,EAAI,IAAIvN,EAAOqJ,cAC5B,eAAJrK,IAAoBuO,EAAI,IAAIvN,EAAO4J,iBAC/B,OAAJ5K,IAAYuO,EAAI,IAAIvN,EAAOyI,gBACvB,WAAJzJ,IAAgBuO,EAAI,IAAIvN,EAAOyK,aAC3B,kBAAJzL,IAAuBuO,EAAI,IAAIvN,EAAOkM,oBAClC,QAAJlN,IAAauO,EAAI,IAAIvN,EAAOiK,UAC/BsD,EAAE7I,SAAS8I,GACXjK,KAAKiJ,OAAO3J,KAAK0K,MAMvBvN,EAAOsM,IAAMA,EAvLf,CAwLGxM,GACH,SAAUE,GACR,aACUA,EAAOkD,IAAjB,IAEIuK,EAAU,SAASC,EAAKnB,GAE1BhJ,KAAKmK,IAAMA,EAEPnB,EAAUA,GAAW,GACzBhJ,KAAKoK,mBAAiD,IAA1BpB,EAAQoB,cAAgCpB,EAAQoB,cAAgB,IAC5FpK,KAAKqK,cAAuC,IAArBrB,EAAQqB,SAA2BrB,EAAQqB,SAAW,EAC7ErK,KAAKsK,cAAuC,IAArBtB,EAAQsB,SAA2BtB,EAAQsB,SAAW,EAC7EtK,KAAKuK,gBAA2C,IAAvBvB,EAAQuB,WAA6BvB,EAAQuB,WAAa,EACnFvK,KAAKwK,YAAmC,IAAnBxB,EAAQwB,OAAyBxB,EAAQwB,OAAS,MAEvExK,KAAKyK,cAAuC,IAArBzB,EAAQyB,SAA2BzB,EAAQyB,SAAW,GAC7EzK,KAAK0K,QAA2B,IAAf1B,EAAQ0B,GAAqB1B,EAAQ0B,GAAK,IAC3D1K,KAAK2K,SAA6B,IAAhB3B,EAAQ2B,IAAsB3B,EAAQ2B,IAAM,KAE9D3K,KAAKZ,EAAI,EACTY,KAAK4K,KAAO,GACZ5K,KAAK6K,KAAO,IAGdX,EAAQ/N,UAAY,CAClB2O,MAAO,SAAS3K,EAAGC,GAEjB,IAAI2K,GAAQ,IAAIC,MAAOC,UACvBjL,KAAKmK,IAAItG,QAAQ1D,GAAG,GACpB,IACI+K,GADM,IAAIF,MAAOC,UACAF,EAGjBI,GADAJ,GAAQ,IAAIC,MAAOC,UACPjL,KAAKmK,IAAIzF,SAAStE,IAC9BgL,EAAgB,EAChBC,EAAgB,EAEhBC,GADM,IAAIN,MAAOC,UACAF,EAGrB,GADA/K,KAAKZ,IACFY,KAAKZ,EAAIY,KAAKuK,YAAe,EAAG,CAEjC,IAAIgB,EAASvL,KAAKmK,IAAIrF,oBAGtB,GAAwB,IAArB9E,KAAK4K,KAAKnN,SAAiC,QAAhBuC,KAAKwK,QAAoBxK,KAAKyK,SAAW,GAKrE,IAAI,IAAIlQ,EAAE,EAAEA,EAAEgR,EAAO9N,OAAOlD,IAC1ByF,KAAK4K,KAAKtL,KAAK7C,EAAOsB,MAAMwN,EAAOhR,GAAGyK,OAAOvH,SAC1B,aAAhBuC,KAAKwK,OACNxK,KAAK6K,KAAKvL,KAAK7C,EAAOsB,MAAMwN,EAAOhR,GAAGyK,OAAOvH,SAE7CuC,KAAK6K,KAAKvL,KAAK,IAMrB,IAAQ/E,EAAE,EAAEA,EAAEgR,EAAO9N,OAAOlD,IAY1B,IAXA,IAAIiR,EAAKD,EAAOhR,GACZ8B,EAAImP,EAAGxG,OACP0D,EAAI8C,EAAGvG,MAGP3B,OAA0C,IAApBkI,EAAGlI,aAA+BkI,EAAGlI,aAAe,EAC1ED,OAA0C,IAApBmI,EAAGnI,aAA+BmI,EAAGnI,aAAe,EAC1EiH,EAAWtK,KAAKsK,SAAWhH,EAC3B+G,EAAWrK,KAAKqK,SAAWhH,EAE3BoI,EAAOpP,EAAEoB,OACLoB,EAAE,EAAEA,EAAE4M,EAAK5M,IAAK,CACtBuM,GAAiBd,EAASjO,EAAEwC,GAAGxC,EAAEwC,GAAG,EACpCwM,GAAiBhB,EAASvN,KAAK4O,IAAIrP,EAAEwC,IACrC,IAAI8M,EAAStB,GAAYhO,EAAEwC,GAAK,EAAI,GAAK,GAGrC+M,GAFStB,EAAYjO,EAAEwC,GAEP8M,EAASjD,EAAE7J,IAAMmB,KAAKuK,WAEtCsB,EAAQ7L,KAAK4K,KAAKrQ,GAClBuR,EAAQ9L,KAAK6K,KAAKtQ,GACtB,GAAmB,YAAhByF,KAAKwK,OAAsB,CAE5BqB,EAAMhN,GAAKgN,EAAMhN,GAAK+M,EAAMA,EAC5B,IAAItK,GAAOtB,KAAKoK,cAAgBtN,KAAKG,KAAK4O,EAAMhN,GAAKmB,KAAK2K,KAAOiB,EACjEvP,EAAEwC,IAAMyC,OACH,GAAmB,eAAhBtB,KAAKwK,OAAyB,CAItCqB,EAAMhN,GAAKmB,KAAK0K,GAAKmB,EAAMhN,IAAM,EAAEmB,KAAK0K,IAAMkB,EAAMA,EAChDtK,GAAOtB,KAAKoK,cAAgBtN,KAAKG,KAAK4O,EAAMhN,GAAKmB,KAAK2K,KAAOiB,EACjEvP,EAAEwC,IAAMyC,OACH,GAAmB,aAAhBtB,KAAKwK,OAAuB,CAEpCqB,EAAMhN,GAAKmB,KAAK0K,GAAKmB,EAAMhN,IAAM,EAAEmB,KAAK0K,IAAMkB,EAAMA,EAChDtK,GAAOxE,KAAKG,MAAM6O,EAAMjN,GAAKmB,KAAK2K,MAAMkB,EAAMhN,GAAKmB,KAAK2K,MAAQiB,EACpEE,EAAMjN,GAAKmB,KAAK0K,GAAKoB,EAAMjN,IAAM,EAAEmB,KAAK0K,IAAMpJ,EAAKA,EACnDjF,EAAEwC,IAAMyC,OAGR,GAAGtB,KAAKyK,SAAW,EAAK,CAElBnJ,EAAKtB,KAAKyK,SAAWoB,EAAMhN,GAAKmB,KAAKoK,cAAgBwB,EACzDC,EAAMhN,GAAKyC,EACXjF,EAAEwC,IAAMyC,OAGRjF,EAAEwC,KAASmB,KAAKoK,cAAgBwB,EAGpClD,EAAE7J,GAAK,GASb,MAAO,CAACqM,SAAUA,EAAUI,SAAUA,EAC9BF,cAAeA,EAAeC,cAAeA,EAC7CF,UAAWA,EAAWY,aAAcZ,EACpC7E,KAAM6E,EAAYE,EAAgBD,KAI9C3O,EAAOyN,QAAUA,EACjBzN,EAAOuP,WAAa9B,EAlItB,CAmIG3N,GAEH,SAAUE,GACR,aAGA,IAAIU,EAAQV,EAAOU,MACfO,EAAQjB,EAAOiB,MACfqL,EAAMtM,EAAOsM,IACbmB,EAAUzN,EAAOyN,QACjB9L,EAAS3B,EAAO2B,OAChBO,EAAWlC,EAAOkC,SAClBK,EAAiBvC,EAAOuC,eACxBO,EAAS9C,EAAO8C,OAChBF,EAAY5C,EAAO4C,UAUnB4M,EAAW,SAASxJ,EAAMyJ,EAAQ1M,GAChCA,EAAMA,GAAO,QACE,IAATiD,IAAwBA,EAAO,SACpB,IAAXyJ,IAA0BA,EAAS,IAG7ClM,KAAKyC,KAAOA,EACZzC,KAAKkM,OAASA,EAGdlM,KAAKmM,YAAc5M,EAAOC,EAAK,cAAe,IAC9CQ,KAAKoM,UAAY7M,EAAOC,EAAK,YAAa,IAC1CQ,KAAKqM,eAAiB9M,EAAOC,EAAK,iBAAkB,IAGpDQ,KAAKsM,WAAa/M,EAAOC,EAAK,aAAc,IAE5CQ,KAAKuM,cAAgBhN,EAAOC,EAAK,gBAAiB,IAGlDQ,KAAKwM,eAAiBjN,EAAOC,EAAK,iBAAkB,IACpDQ,KAAKyM,eAAiBlN,EAAOC,EAAK,iBAAkB,KACpDQ,KAAK0M,aAAenN,EAAOC,EAAK,gBAAiB,GACjDQ,KAAK2M,aAAepN,EAAOC,EAAK,eAAgB,GAChDQ,KAAK4M,kBAAoBrN,EAAOC,EAAK,qBAAsB,GAC3DQ,KAAK6M,kBAAoBtN,EAAOC,EAAK,oBAAqB,GAC1DQ,KAAK8M,aAAevN,EAAOC,EAAK,eAAgB,IAChDQ,KAAK+M,aAAexN,EAAOC,EAAK,eAAgB,IAChDQ,KAAKgN,YAAczN,EAAOC,EAAK,cAAe,GAC9CQ,KAAKiN,YAAc1N,EAAOC,EAAK,cAAe,IAG9CQ,KAAKkN,MAAQ,GACblN,KAAKmN,WAAa,GAClBnN,KAAKoN,qBAAuB,GAC5BpN,KAAKqN,cAAgBhO,EAAU6M,GAC/BlM,KAAKsN,KAAO,EACZtN,KAAKuN,OAAS,EAGdvN,KAAKwN,qBAAuB,KAC5BxN,KAAKyN,sBAAwB,KAG1BzN,KAAKyC,KAAKhF,OAAS,IACpBuC,KAAK0N,cACL1N,KAAK2N,qBAIT1B,EAAS9P,UAAY,CAGnBuR,YAAa,WACX,IAAI5G,EAAI9G,KAAKyC,KAAKhF,OACdmQ,EAAY9Q,KAAKa,MAAMqC,KAAKmM,YAAcrF,GAC9C9G,KAAKkN,MAAQ,GACb,IAAI,IAAI3S,EAAE,EAAEA,EAAEyF,KAAKoM,UAAU7R,IAAK,CAChC,IAAI8B,EAAIsC,EAASmI,GACjB9G,KAAKkN,MAAM5N,KAAK,CAACuO,SAAUxR,EAAEyR,MAAM,EAAGF,GAAYG,QAAS1R,EAAEyR,MAAMF,EAAW9G,OAKlFkH,gBAAiB,WACf,IAAIC,EAAcjO,KAAKyC,KAAK,GAAGpE,EAAEZ,OAC7B8L,EAAcvJ,KAAKqN,cAAc5P,OAGjCyQ,EAAa,GACjBA,EAAW5O,KAAK,CAAC8J,KAAK,QAAS7F,OAAO,EAAGC,OAAO,EAAGV,UAAWmL,IAE9D,IADA,IAAIE,EAAKnP,EAAe,CAAC,EAAE,EAAE,EAAE,GAAI,CAAC,GAAK,GAAK,GAAK,KAC3CD,EAAE,EAAEA,EAAEoP,EAAGpP,IAAK,CACpB,IAAIqP,EAAK1Q,EAAMsC,KAAKgN,YAAahN,KAAKiN,aAClCpD,EAAM,CAAC,OAAO,SAAS,QAAQnM,EAAM,EAAE,IAC3C,GAAGP,EAAM,EAAE,GAAG,GAAK,CACjB,IAAIkR,EAAKvR,KAAKC,SACdmR,EAAW5O,KAAK,CAAC8J,KAAK,KAAMjE,YAAaiJ,EAAI5E,WAAYK,EAAKpC,UAAW4G,SAEzEH,EAAW5O,KAAK,CAAC8J,KAAK,KAAMjE,YAAaiJ,EAAI5E,WAAYK,IAG7DqE,EAAW5O,KAAK,CAAC8J,KAAK,UAAWG,YAAaA,IAC9C,IAAIY,EAAM,IAAIpB,EACdoB,EAAIjB,WAAWgF,GAGf,IAKII,EALAC,EAAK7Q,EAAMsC,KAAKwM,eAAgBxM,KAAKyM,gBACrC+B,EAAK1R,KAAKuL,IAAI,GAAIlL,EAAM6C,KAAK0M,aAAc1M,KAAK2M,eAChD8B,EAAK3R,KAAKuL,IAAI,GAAIlL,EAAM6C,KAAK4M,kBAAmB5M,KAAK6M,oBACrD6B,EAAMvR,EAAM6C,KAAK8M,aAAc9M,KAAK+M,cACpC4B,EAAKxR,EAAM,EAAE,GAUbyR,EAAU,IAAI1E,EAAQC,EAPxBmE,EADCK,EAAG,IACU,CAACnE,OAAO,WAAYD,WAAWgE,EAAIjE,SAASkE,GAClDG,EAAG,IACG,CAACnE,OAAO,UAAWJ,cAAeqE,EAAIlE,WAAWgE,EAAIjE,SAASkE,GAE9D,CAAChE,OAAO,MAAOJ,cAAeqE,EAAIhE,SAAUiE,EAAKnE,WAAWgE,EAAIjE,SAASkE,IAKrFK,EAAO,CACX,IAAW,GACX,KAAY,GAKZ,OAJAA,EAAKX,WAAaA,EAClBW,EAAKP,YAAcA,EACnBO,EAAK1E,IAAMA,EACX0E,EAAKD,QAAUA,EACRC,GAITlB,iBAAkB,WAChB3N,KAAKmN,WAAa,GAClB,IAAI,IAAI5S,EAAE,EAAEA,EAAEyF,KAAKqM,eAAe9R,IAAK,CACrC,IAAIsU,EAAO7O,KAAKgO,kBAChBhO,KAAKmN,WAAW7N,KAAKuP,KAIzBC,KAAM,WAGJ9O,KAAKsN,OAKL,IAFA,IAAIyB,EAAO/O,KAAKkN,MAAMlN,KAAKuN,QACvByB,EAASD,EAAKlB,SAASnQ,EAAM,EAAGqR,EAAKlB,SAASpQ,SAC1C2B,EAAE,EAAEA,EAAEY,KAAKmN,WAAW1P,OAAO2B,IAAK,CACxC,IAAIe,EAAIH,KAAKyC,KAAKuM,GACdxU,EAAIwF,KAAKkM,OAAO8C,GACpBhP,KAAKmN,WAAW/N,GAAGwP,QAAQ9D,MAAM3K,EAAG3F,GAItC,IAAIyU,EAAWjP,KAAKsM,WAAayC,EAAKlB,SAASpQ,OAC/C,GAAGuC,KAAKsN,MAAQ2B,EAAU,CAGxB,IAAIC,EAAUlP,KAAKmP,gBACnB,IAAQ/P,EAAE,EAAEA,EAAEY,KAAKmN,WAAW1P,OAAO2B,IAAK,EACpCxE,EAAIoF,KAAKmN,WAAW/N,IACtBgQ,IAAI9P,KAAK4P,EAAQ9P,IACnBxE,EAAEyU,MAAQH,EAAQ9P,GASpB,GAPAY,KAAKsN,KAAO,EACZtN,KAAKuN,SAE4B,OAA9BvN,KAAKwN,sBACNxN,KAAKwN,uBAGJxN,KAAKuN,QAAUvN,KAAKkN,MAAMzP,OAAQ,CAGnC,IAAQ2B,EAAE,EAAEA,EAAEY,KAAKmN,WAAW1P,OAAO2B,IACnCY,KAAKoN,qBAAqB9N,KAAKU,KAAKmN,WAAW/N,IAGjDY,KAAKoN,qBAAqBkC,MAAK,SAASlS,EAAGC,GACzC,OAAQD,EAAEiS,KAAOjS,EAAEgS,IAAI3R,OACfJ,EAAEgS,KAAOhS,EAAE+R,IAAI3R,QACf,EAAI,KAKXuC,KAAKoN,qBAAqB3P,OAAS,EAAIuC,KAAKuM,gBAC7CvM,KAAKoN,qBAAuBpN,KAAKoN,qBAAqBU,MAAM,EAAG,EAAI9N,KAAKuM,gBAExC,OAA/BvM,KAAKyN,uBACNzN,KAAKyN,wBAEPzN,KAAK2N,mBACL3N,KAAKuN,OAAS,OAGd,IAAQnO,EAAE,EAAEA,EAAEY,KAAKmN,WAAW1P,OAAO2B,IAAK,CACxC,IAAIxE,EAAIoF,KAAKmN,WAAW/N,GACpB+K,EAAM,IAAIpB,EACdoB,EAAIjB,WAAWtO,EAAEsT,YACjB,IAAIU,EAAU,IAAI1E,EAAQC,EAAKvP,EAAE0T,aACjC1T,EAAEuP,IAAMA,EACRvP,EAAEgU,QAAUA,KAMpBO,cAAe,WAKb,IAFA,IAAII,EAAO,GACPR,EAAO/O,KAAKkN,MAAMlN,KAAKuN,QACnBnO,EAAE,EAAEA,EAAEY,KAAKmN,WAAW1P,OAAO2B,IAAK,CAGxC,IAFA,IAAI+K,EAAMnK,KAAKmN,WAAW/N,GAAG+K,IACzBnN,EAAI,EACA+B,EAAE,EAAEA,EAAEgQ,EAAKhB,QAAQtQ,OAAOsB,IAAK,CACrC,IAAIoB,EAAIH,KAAKyC,KAAKsM,EAAKhB,QAAQhP,IAC3BvE,EAAIwF,KAAKkM,OAAO6C,EAAKhB,QAAQhP,IACjCoL,EAAItG,QAAQ1D,GAEZnD,GADWmN,EAAIJ,kBACAvP,EAAI,EAAM,EAE3BwC,GAAK+R,EAAKhB,QAAQtQ,OAClB8R,EAAKjQ,KAAKtC,GAEZ,OAAOuS,GAMTC,aAAc,SAAS/M,GAGrB,IAEIgN,EAAMzT,EAFN0T,EAAK5S,KAAKqL,IAAInI,KAAKuM,cAAevM,KAAKoN,qBAAqB3P,QAChE,GAAU,IAAPiS,EAAY,OAAO,IAAInT,EAAUoD,IAAI,EAAE,EAAE,GAE5C,IAAI,IAAId,EAAE,EAAEA,EAAE6Q,EAAG7Q,IAAK,CACpB,IACIsB,EADMH,KAAKoN,qBAAqBvO,GAAGsL,IAC3BtG,QAAQpB,GACpB,GAAO,IAAJ5D,EACD4Q,EAAOtP,EACPnE,EAAImE,EAAE9B,EAAEZ,YAGR,IAAI,IAAI5C,EAAE,EAAEA,EAAEmB,EAAEnB,IACd4U,EAAKpR,EAAExD,IAAMsF,EAAE9B,EAAExD,GAKvB,IAAQA,EAAE,EAAEA,EAAEmB,EAAEnB,IACd4U,EAAKpR,EAAExD,IAAMmB,EAEf,OAAOyT,GAGTE,QAAS,SAASlN,GAChB,IAAIgN,EAAOzP,KAAKwP,aAAa/M,GAC7B,GAAqB,IAAlBgN,EAAKpR,EAAEZ,OACR,IACImS,EADQxR,EAAOqR,EAAKpR,GACIG,UAExBoR,GAAmB,EAEzB,OAAOA,GAIT3O,OAAQ,WAKN,IAHA,IAAIyO,EAAK5S,KAAKqL,IAAInI,KAAKuM,cAAevM,KAAKoN,qBAAqB3P,QAC5DyD,EAAO,CACX,KAAY,IACJ3G,EAAE,EAAEA,EAAEmV,EAAGnV,IACf2G,EAAK2O,KAAKvQ,KAAKU,KAAKoN,qBAAqB7S,GAAG4P,IAAIlJ,UAElD,OAAOC,GAGTC,SAAU,SAASD,GACjBlB,KAAKuM,cAAgBrL,EAAK2O,KAAKpS,OAC/BuC,KAAKoN,qBAAuB,GAC5B,IAAI,IAAI7S,EAAE,EAAEA,EAAEyF,KAAKuM,cAAchS,IAAK,CACpC,IAAI4P,EAAM,IAAIpB,EACdoB,EAAIhJ,SAASD,EAAK2O,KAAKtV,IACvB,IAAIuV,EAAkB,GACtBA,EAAgB3F,IAAMA,EACtBnK,KAAKoN,qBAAqB9N,KAAKwQ,KAMnCC,aAAc,SAAS9L,GAAKjE,KAAKwN,qBAAuBvJ,GAExD+L,cAAe,SAAS/L,GAAKjE,KAAKyN,sBAAwBxJ,IAI5DxH,EAAOwP,SAAWA,EAlTpB,CAmTG1P,GACH,SAAU0T,GACR,kBAC+D,IAAnB3V,EAAOD,QACjD6V,OAAOC,OAASF,EAEhB3V,EAAOD,QAAU4V,EALrB,CAOG1T,I,+CCrpEQ6T,EAAsB,EAAE,EAAG,GCElCjO,EAAMkO,KACNC,EAAcD,KAAKE,YACvBpO,EAAIqO,iBAAiB,WAAW,SAAUC,GACtC,IAAIC,EAAeC,KAAKC,MAAMH,EAAQhO,MAElCoO,EAAY,GAChB,OAFWH,EAAatH,MAGpB,IAAK,QACD,IAAI0H,EAAY,IAAIC,EAAA,IACpBD,EAAU3P,SAASuP,EAAaM,OAChC,IAAIC,EAAcP,EAAaQ,MAC3BC,EAAeT,EAAaU,OAC5BC,EAAYX,EAAaxI,IACzBoJ,EAAcZ,EAAavI,IAC3BoJ,EAAiB,GACrBF,EAAUG,SAAQ,SAAUC,EAAMC,GAC9BH,EAAeG,GAASD,EAAOH,EAAYI,MAK/C,IAHA,IAAIC,EAAYV,EAAYxT,OACxB0C,EAAI,IAAI4Q,EAAA,IAAc,EAAG,EAAGY,GAC5B/C,EAAU,IAAImC,EAAA,QAAkBD,EAAW,CAAEtG,OAAQ,UAAWF,SAAU,KAAOD,SAAU,KAAOE,WAAY,KACzGqH,EAAY,EAAGA,EAAY,GAAIA,IACpC,IAAK,IAAIrX,EAAI,EAAGA,EAAI0W,EAAYxT,OAAQlD,IAAK,CAEzC,IADA,IAAIsX,EAAa,GACRhT,EAAI,EAAGA,EAAIoS,EAAY1W,GAAGkD,OAAQoB,IACd,GAArB0S,EAAe1S,GACfgT,EAAWhT,IAAMoS,EAAY1W,GAAGsE,GAAKyS,EAAYzS,IAAM0S,EAAe1S,GAAK,GAAK,EAGhFgT,EAAWhT,GAAKoS,EAAY1W,GAAGsE,GAGvCsB,EAAE9B,EAAIwT,EACNjD,EAAQ9D,MAAM3K,EAAGgR,EAAa5W,IAGtCsW,EAAe,IAAIC,EAAU7P,SAC7B4P,EAAgB,KAAI,QACpBA,EAAc,GAAIH,EAAaoB,GAC/BxB,EAAYO,GACZ,MACJ,IAAK,OACD,IAAIkB,EAAW,IAAIhB,EAAA,IACnBgB,EAAS5Q,SAASuP,EAAaM,OAI/B,IAHA,IAAIlC,GAAQsB,EAAoB,GAAKA,EAAoB,ID5CnC,GC6ClB4B,EAAY,GACZC,EAAW,EACN3Q,EAAK8O,EAAoB,GAAI9O,EAAK8O,EAAoB,GAAI9O,EAAK4Q,QAAQ5Q,EAAKwN,GAAMqD,QAAQ,IAAK,CACpGH,EAAUC,GAAY,GAwBtB,IAvBA,IAAIG,EAAU,SAAU7Q,GACpB,IAAI8Q,EAAY,IAAItB,EAAA,IAAc,CAACzP,EAAIC,IACnC+Q,EAASP,EAASlO,QAAQwO,GAC1BE,EAAa,EACbC,EAAY,EAAIF,EAAOjU,EAAEZ,OACzBgV,EAAUD,EACV9B,EAAagC,eACbD,EAAU,IAGVH,EAAOjU,EAAEmT,SAAQ,SAAUmB,EAAQC,GAC3BD,EAASH,IACTD,EAAaK,EACbH,EAAUE,MAGlBF,GAAWA,EAAUD,IAAc,EAAIA,IAE3CR,EAAUC,GAAUY,QAAQ,CACxBD,YAAaL,EACbE,QAASA,KAGRlR,EAAK6O,EAAoB,GAAI7O,EAAK6O,EAAoB,GAAI7O,EAAK2Q,QAAQ3Q,EAAKuN,GAAMqD,QAAQ,IAC/FC,EAAQ7Q,GAEZ0Q,IAEJpB,EAAuB,YAAImB,EAC3BnB,EAAgB,KAAI,OACpBA,EAAc,GAAIH,EAAaoB,GAC/BxB,EAAYO","file":"77aca448a1dca61cb65d.worker.js","sourcesContent":[" \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"/\";\n\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 1);\n","var convnetjs = convnetjs || { REVISION: 'ALPHA' };\n(function(global) {\n  \"use strict\";\n\n  // Random number utilities\n  var return_v = false;\n  var v_val = 0.0;\n  var gaussRandom = function() {\n    if(return_v) { \n      return_v = false;\n      return v_val; \n    }\n    var u = 2*Math.random()-1;\n    var v = 2*Math.random()-1;\n    var r = u*u + v*v;\n    if(r == 0 || r > 1) return gaussRandom();\n    var c = Math.sqrt(-2*Math.log(r)/r);\n    v_val = v*c; // cache this\n    return_v = true;\n    return u*c;\n  }\n  var randf = function(a, b) { return Math.random()*(b-a)+a; }\n  var randi = function(a, b) { return Math.floor(Math.random()*(b-a)+a); }\n  var randn = function(mu, std){ return mu+gaussRandom()*std; }\n\n  // Array utilities\n  var zeros = function(n) {\n    if(typeof(n)==='undefined' || isNaN(n)) { return []; }\n    if(typeof ArrayBuffer === 'undefined') {\n      // lacking browser support\n      var arr = new Array(n);\n      for(var i=0;i<n;i++) { arr[i]= 0; }\n      return arr;\n    } else {\n      return new Float64Array(n);\n    }\n  }\n\n  var arrContains = function(arr, elt) {\n    for(var i=0,n=arr.length;i<n;i++) {\n      if(arr[i]===elt) return true;\n    }\n    return false;\n  }\n\n  var arrUnique = function(arr) {\n    var b = [];\n    for(var i=0,n=arr.length;i<n;i++) {\n      if(!arrContains(b, arr[i])) {\n        b.push(arr[i]);\n      }\n    }\n    return b;\n  }\n\n  // return max and min of a given non-empty array.\n  var maxmin = function(w) {\n    if(w.length === 0) { return {}; } // ... ;s\n    var maxv = w[0];\n    var minv = w[0];\n    var maxi = 0;\n    var mini = 0;\n    var n = w.length;\n    for(var i=1;i<n;i++) {\n      if(w[i] > maxv) { maxv = w[i]; maxi = i; } \n      if(w[i] < minv) { minv = w[i]; mini = i; } \n    }\n    return {maxi: maxi, maxv: maxv, mini: mini, minv: minv, dv:maxv-minv};\n  }\n\n  // create random permutation of numbers, in range [0...n-1]\n  var randperm = function(n) {\n    var i = n,\n        j = 0,\n        temp;\n    var array = [];\n    for(var q=0;q<n;q++)array[q]=q;\n    while (i--) {\n        j = Math.floor(Math.random() * (i+1));\n        temp = array[i];\n        array[i] = array[j];\n        array[j] = temp;\n    }\n    return array;\n  }\n\n  // sample from list lst according to probabilities in list probs\n  // the two lists are of same size, and probs adds up to 1\n  var weightedSample = function(lst, probs) {\n    var p = randf(0, 1.0);\n    var cumprob = 0.0;\n    for(var k=0,n=lst.length;k<n;k++) {\n      cumprob += probs[k];\n      if(p < cumprob) { return lst[k]; }\n    }\n  }\n\n  // syntactic sugar function for getting default parameter values\n  var getopt = function(opt, field_name, default_value) {\n    return typeof opt[field_name] !== 'undefined' ? opt[field_name] : default_value;\n  }\n\n  global.randf = randf;\n  global.randi = randi;\n  global.randn = randn;\n  global.zeros = zeros;\n  global.maxmin = maxmin;\n  global.randperm = randperm;\n  global.weightedSample = weightedSample;\n  global.arrUnique = arrUnique;\n  global.arrContains = arrContains;\n  global.getopt = getopt;\n  \n})(convnetjs);\n(function(global) {\n  \"use strict\";\n\n  // Vol is the basic building block of all data in a net.\n  // it is essentially just a 3D volume of numbers, with a\n  // width (sx), height (sy), and depth (depth).\n  // it is used to hold data for all filters, all volumes,\n  // all weights, and also stores all gradients w.r.t. \n  // the data. c is optionally a value to initialize the volume\n  // with. If c is missing, fills the Vol with random numbers.\n  var Vol = function(sx, sy, depth, c) {\n    // this is how you check if a variable is an array. Oh, Javascript :)\n    if(Object.prototype.toString.call(sx) === '[object Array]') {\n      // we were given a list in sx, assume 1D volume and fill it up\n      this.sx = 1;\n      this.sy = 1;\n      this.depth = sx.length;\n      // we have to do the following copy because we want to use\n      // fast typed arrays, not an ordinary javascript array\n      this.w = global.zeros(this.depth);\n      this.dw = global.zeros(this.depth);\n      for(var i=0;i<this.depth;i++) {\n        this.w[i] = sx[i];\n      }\n    } else {\n      // we were given dimensions of the vol\n      this.sx = sx;\n      this.sy = sy;\n      this.depth = depth;\n      var n = sx*sy*depth;\n      this.w = global.zeros(n);\n      this.dw = global.zeros(n);\n      if(typeof c === 'undefined') {\n        // weight normalization is done to equalize the output\n        // variance of every neuron, otherwise neurons with a lot\n        // of incoming connections have outputs of larger variance\n        var scale = Math.sqrt(1.0/(sx*sy*depth));\n        for(var i=0;i<n;i++) { \n          this.w[i] = global.randn(0.0, scale);\n        }\n      } else {\n        for(var i=0;i<n;i++) { \n          this.w[i] = c;\n        }\n      }\n    }\n  }\n\n  Vol.prototype = {\n    get: function(x, y, d) { \n      var ix=((this.sx * y)+x)*this.depth+d;\n      return this.w[ix];\n    },\n    set: function(x, y, d, v) { \n      var ix=((this.sx * y)+x)*this.depth+d;\n      this.w[ix] = v; \n    },\n    add: function(x, y, d, v) { \n      var ix=((this.sx * y)+x)*this.depth+d;\n      this.w[ix] += v; \n    },\n    get_grad: function(x, y, d) { \n      var ix = ((this.sx * y)+x)*this.depth+d;\n      return this.dw[ix]; \n    },\n    set_grad: function(x, y, d, v) { \n      var ix = ((this.sx * y)+x)*this.depth+d;\n      this.dw[ix] = v; \n    },\n    add_grad: function(x, y, d, v) { \n      var ix = ((this.sx * y)+x)*this.depth+d;\n      this.dw[ix] += v; \n    },\n    cloneAndZero: function() { return new Vol(this.sx, this.sy, this.depth, 0.0)},\n    clone: function() {\n      var V = new Vol(this.sx, this.sy, this.depth, 0.0);\n      var n = this.w.length;\n      for(var i=0;i<n;i++) { V.w[i] = this.w[i]; }\n      return V;\n    },\n    addFrom: function(V) { for(var k=0;k<this.w.length;k++) { this.w[k] += V.w[k]; }},\n    addFromScaled: function(V, a) { for(var k=0;k<this.w.length;k++) { this.w[k] += a*V.w[k]; }},\n    setConst: function(a) { for(var k=0;k<this.w.length;k++) { this.w[k] = a; }},\n\n    toJSON: function() {\n      // todo: we may want to only save d most significant digits to save space\n      var json = {}\n      json.sx = this.sx; \n      json.sy = this.sy;\n      json.depth = this.depth;\n      json.w = this.w;\n      return json;\n      // we wont back up gradients to save space\n    },\n    fromJSON: function(json) {\n      this.sx = json.sx;\n      this.sy = json.sy;\n      this.depth = json.depth;\n\n      var n = this.sx*this.sy*this.depth;\n      this.w = global.zeros(n);\n      this.dw = global.zeros(n);\n      // copy over the elements.\n      for(var i=0;i<n;i++) {\n        this.w[i] = json.w[i];\n      }\n    }\n  }\n\n  global.Vol = Vol;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // Volume utilities\n  // intended for use with data augmentation\n  // crop is the size of output\n  // dx,dy are offset wrt incoming volume, of the shift\n  // fliplr is boolean on whether we also want to flip left<->right\n  var augment = function(V, crop, dx, dy, fliplr) {\n    // note assumes square outputs of size crop x crop\n    if(typeof(fliplr)==='undefined') var fliplr = false;\n    if(typeof(dx)==='undefined') var dx = global.randi(0, V.sx - crop);\n    if(typeof(dy)==='undefined') var dy = global.randi(0, V.sy - crop);\n    \n    // randomly sample a crop in the input volume\n    var W;\n    if(crop !== V.sx || dx!==0 || dy!==0) {\n      W = new Vol(crop, crop, V.depth, 0.0);\n      for(var x=0;x<crop;x++) {\n        for(var y=0;y<crop;y++) {\n          if(x+dx<0 || x+dx>=V.sx || y+dy<0 || y+dy>=V.sy) continue; // oob\n          for(var d=0;d<V.depth;d++) {\n           W.set(x,y,d,V.get(x+dx,y+dy,d)); // copy data over\n          }\n        }\n      }\n    } else {\n      W = V;\n    }\n\n    if(fliplr) {\n      // flip volume horziontally\n      var W2 = W.cloneAndZero();\n      for(var x=0;x<W.sx;x++) {\n        for(var y=0;y<W.sy;y++) {\n          for(var d=0;d<W.depth;d++) {\n           W2.set(x,y,d,W.get(W.sx - x - 1,y,d)); // copy data over\n          }\n        }\n      }\n      W = W2; //swap\n    }\n    return W;\n  }\n\n  // img is a DOM element that contains a loaded image\n  // returns a Vol of size (W, H, 4). 4 is for RGBA\n  var img_to_vol = function(img, convert_grayscale) {\n\n    if(typeof(convert_grayscale)==='undefined') var convert_grayscale = false;\n\n    var canvas = document.createElement('canvas');\n    canvas.width = img.width;\n    canvas.height = img.height;\n    var ctx = canvas.getContext(\"2d\");\n\n    // due to a Firefox bug\n    try {\n      ctx.drawImage(img, 0, 0);\n    } catch (e) {\n      if (e.name === \"NS_ERROR_NOT_AVAILABLE\") {\n        // sometimes happens, lets just abort\n        return false;\n      } else {\n        throw e;\n      }\n    }\n\n    try {\n      var img_data = ctx.getImageData(0, 0, canvas.width, canvas.height);\n    } catch (e) {\n      if(e.name === 'IndexSizeError') {\n        return false; // not sure what causes this sometimes but okay abort\n      } else {\n        throw e;\n      }\n    }\n\n    // prepare the input: get pixels and normalize them\n    var p = img_data.data;\n    var W = img.width;\n    var H = img.height;\n    var pv = []\n    for(var i=0;i<p.length;i++) {\n      pv.push(p[i]/255.0-0.5); // normalize image pixels to [-0.5, 0.5]\n    }\n    var x = new Vol(W, H, 4, 0.0); //input volume (image)\n    x.w = pv;\n\n    if(convert_grayscale) {\n      // flatten into depth=1 array\n      var x1 = new Vol(W, H, 1, 0.0);\n      for(var i=0;i<W;i++) {\n        for(var j=0;j<H;j++) {\n          x1.set(i,j,0,x.get(i,j,0));\n        }\n      }\n      x = x1;\n    }\n\n    return x;\n  }\n  \n  global.augment = augment;\n  global.img_to_vol = img_to_vol;\n\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // This file contains all layers that do dot products with input,\n  // but usually in a different connectivity pattern and weight sharing\n  // schemes: \n  // - FullyConn is fully connected dot products \n  // - ConvLayer does convolutions (so weight sharing spatially)\n  // putting them together in one file because they are very similar\n  var ConvLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    this.out_depth = opt.filters;\n    this.sx = opt.sx; // filter size. Should be odd if possible, it's cleaner.\n    this.in_depth = opt.in_depth;\n    this.in_sx = opt.in_sx;\n    this.in_sy = opt.in_sy;\n    \n    // optional\n    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;\n    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 1; // stride at which we apply filters to input volume\n    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume\n    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;\n    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0;\n\n    // computed\n    // note we are doing floor, so if the strided convolution of the filter doesnt fit into the input\n    // volume exactly, the output volume will be trimmed and not contain the (incomplete) computed\n    // final application.\n    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);\n    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);\n    this.layer_type = 'conv';\n\n    // initializations\n    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;\n    this.filters = [];\n    for(var i=0;i<this.out_depth;i++) { this.filters.push(new Vol(this.sx, this.sy, this.in_depth)); }\n    this.biases = new Vol(1, 1, this.out_depth, bias);\n  }\n  ConvLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n      for(var d=0;d<this.out_depth;d++) {\n        var f = this.filters[d];\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n\n            // convolve centered at this particular location\n            // could be bit more efficient, going for correctness first\n            var a = 0.0;\n            for(var fx=0;fx<f.sx;fx++) {\n              for(var fy=0;fy<f.sy;fy++) {\n                for(var fd=0;fd<f.depth;fd++) {\n                  var oy = y+fy; // coordinates in the original input array coordinates\n                  var ox = x+fx;\n                  if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {\n                    //a += f.get(fx, fy, fd) * V.get(ox, oy, fd);\n                    // avoid function call overhead for efficiency, compromise modularity :(\n                    a += f.w[((f.sx * fy)+fx)*f.depth+fd] * V.w[((V.sx * oy)+ox)*V.depth+fd];\n                  }\n                }\n              }\n            }\n            a += this.biases.w[d];\n            A.set(ax, ay, d, a);\n          }\n        }\n      }\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function() { \n\n      // compute gradient wrt weights, biases and input data\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt bottom data, we're about to fill it\n      for(var d=0;d<this.out_depth;d++) {\n        var f = this.filters[d];\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n            // convolve and add up the gradients. \n            // could be more efficient, going for correctness first\n            var chain_grad = this.out_act.get_grad(ax,ay,d); // gradient from above, from chain rule\n            for(var fx=0;fx<f.sx;fx++) {\n              for(var fy=0;fy<f.sy;fy++) {\n                for(var fd=0;fd<f.depth;fd++) {\n                  var oy = y+fy;\n                  var ox = x+fx;\n                  if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {\n                    // forward prop calculated: a += f.get(fx, fy, fd) * V.get(ox, oy, fd);\n                    //f.add_grad(fx, fy, fd, V.get(ox, oy, fd) * chain_grad);\n                    //V.add_grad(ox, oy, fd, f.get(fx, fy, fd) * chain_grad);\n\n                    // avoid function call overhead and use Vols directly for efficiency\n                    var ix1 = ((V.sx * oy)+ox)*V.depth+fd;\n                    var ix2 = ((f.sx * fy)+fx)*f.depth+fd;\n                    f.dw[ix2] += V.w[ix1]*chain_grad;\n                    V.dw[ix1] += f.w[ix2]*chain_grad;\n                  }\n                }\n              }\n            }\n            this.biases.dw[d] += chain_grad;\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      var response = [];\n      for(var i=0;i<this.out_depth;i++) {\n        response.push({params: this.filters[i].w, grads: this.filters[i].dw, l2_decay_mul: this.l2_decay_mul, l1_decay_mul: this.l1_decay_mul});\n      }\n      response.push({params: this.biases.w, grads: this.biases.dw, l1_decay_mul: 0.0, l2_decay_mul: 0.0});\n      return response;\n    },\n    toJSON: function() {\n      var json = {};\n      json.sx = this.sx; // filter size in x, y dims\n      json.sy = this.sy;\n      json.stride = this.stride;\n      json.in_depth = this.in_depth;\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.l1_decay_mul = this.l1_decay_mul;\n      json.l2_decay_mul = this.l2_decay_mul;\n      json.pad = this.pad;\n      json.filters = [];\n      for(var i=0;i<this.filters.length;i++) {\n        json.filters.push(this.filters[i].toJSON());\n      }\n      json.biases = this.biases.toJSON();\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.sx = json.sx; // filter size in x, y dims\n      this.sy = json.sy;\n      this.stride = json.stride;\n      this.in_depth = json.in_depth; // depth of input volume\n      this.filters = [];\n      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;\n      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;\n      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0;\n      for(var i=0;i<json.filters.length;i++) {\n        var v = new Vol(0,0,0,0);\n        v.fromJSON(json.filters[i]);\n        this.filters.push(v);\n      }\n      this.biases = new Vol(0,0,0,0);\n      this.biases.fromJSON(json.biases);\n    }\n  }\n\n  var FullyConnLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    // ok fine we will allow 'filters' as the word as well\n    this.out_depth = typeof opt.num_neurons !== 'undefined' ? opt.num_neurons : opt.filters;\n\n    // optional \n    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;\n    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0;\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'fc';\n\n    // initializations\n    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;\n    this.filters = [];\n    for(var i=0;i<this.out_depth ;i++) { this.filters.push(new Vol(1, 1, this.num_inputs)); }\n    this.biases = new Vol(1, 1, this.out_depth, bias);\n  }\n\n  FullyConnLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var A = new Vol(1, 1, this.out_depth, 0.0);\n      var Vw = V.w;\n      for(var i=0;i<this.out_depth;i++) {\n        var a = 0.0;\n        var wi = this.filters[i].w;\n        for(var d=0;d<this.num_inputs;d++) {\n          a += Vw[d] * wi[d]; // for efficiency use Vols directly for now\n        }\n        a += this.biases.w[i];\n        A.w[i] = a;\n      }\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out the gradient in input Vol\n      \n      // compute gradient wrt weights and data\n      for(var i=0;i<this.out_depth;i++) {\n        var tfi = this.filters[i];\n        var chain_grad = this.out_act.dw[i];\n        for(var d=0;d<this.num_inputs;d++) {\n          V.dw[d] += tfi.w[d]*chain_grad; // grad wrt input data\n          tfi.dw[d] += V.w[d]*chain_grad; // grad wrt params\n        }\n        this.biases.dw[i] += chain_grad;\n      }\n    },\n    getParamsAndGrads: function() {\n      var response = [];\n      for(var i=0;i<this.out_depth;i++) {\n        response.push({params: this.filters[i].w, grads: this.filters[i].dw, l1_decay_mul: this.l1_decay_mul, l2_decay_mul: this.l2_decay_mul});\n      }\n      response.push({params: this.biases.w, grads: this.biases.dw, l1_decay_mul: 0.0, l2_decay_mul: 0.0});\n      return response;\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      json.l1_decay_mul = this.l1_decay_mul;\n      json.l2_decay_mul = this.l2_decay_mul;\n      json.filters = [];\n      for(var i=0;i<this.filters.length;i++) {\n        json.filters.push(this.filters[i].toJSON());\n      }\n      json.biases = this.biases.toJSON();\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;\n      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;\n      this.filters = [];\n      for(var i=0;i<json.filters.length;i++) {\n        var v = new Vol(0,0,0,0);\n        v.fromJSON(json.filters[i]);\n        this.filters.push(v);\n      }\n      this.biases = new Vol(0,0,0,0);\n      this.biases.fromJSON(json.biases);\n    }\n  }\n\n  global.ConvLayer = ConvLayer;\n  global.FullyConnLayer = FullyConnLayer;\n  \n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  var PoolLayer = function(opt) {\n\n    var opt = opt || {};\n\n    // required\n    this.sx = opt.sx; // filter size\n    this.in_depth = opt.in_depth;\n    this.in_sx = opt.in_sx;\n    this.in_sy = opt.in_sy;\n\n    // optional\n    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;\n    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 2;\n    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume\n\n    // computed\n    this.out_depth = this.in_depth;\n    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);\n    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);\n    this.layer_type = 'pool';\n    // store switches for x,y coordinates for where the max comes from, for each output neuron\n    this.switchx = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n    this.switchy = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n  }\n\n  PoolLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n      \n      var n=0; // a counter for switches\n      for(var d=0;d<this.out_depth;d++) {\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n\n            // convolve centered at this particular location\n            var a = -99999; // hopefully small enough ;\\\n            var winx=-1,winy=-1;\n            for(var fx=0;fx<this.sx;fx++) {\n              for(var fy=0;fy<this.sy;fy++) {\n                var oy = y+fy;\n                var ox = x+fx;\n                if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {\n                  var v = V.get(ox, oy, d);\n                  // perform max pooling and store pointers to where\n                  // the max came from. This will speed up backprop \n                  // and can help make nice visualizations in future\n                  if(v > a) { a = v; winx=ox; winy=oy;}\n                }\n              }\n            }\n            this.switchx[n] = winx;\n            this.switchy[n] = winy;\n            n++;\n            A.set(ax, ay, d, a);\n          }\n        }\n      }\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function() { \n      // pooling layers have no parameters, so simply compute \n      // gradient wrt data here\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n      var A = this.out_act; // computed in forward pass \n\n      var n = 0;\n      for(var d=0;d<this.out_depth;d++) {\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n\n            var chain_grad = this.out_act.get_grad(ax,ay,d);\n            V.add_grad(this.switchx[n], this.switchy[n], d, chain_grad);\n            n++;\n\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.sx = this.sx;\n      json.sy = this.sy;\n      json.stride = this.stride;\n      json.in_depth = this.in_depth;\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.pad = this.pad;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.sx = json.sx;\n      this.sy = json.sy;\n      this.stride = json.stride;\n      this.in_depth = json.in_depth;\n      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0; // backwards compatibility\n      this.switchx = global.zeros(this.out_sx*this.out_sy*this.out_depth); // need to re-init these appropriately\n      this.switchy = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n    }\n  }\n\n  global.PoolLayer = PoolLayer;\n\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  var InputLayer = function(opt) {\n    var opt = opt || {};\n\n    // this is a bit silly but lets allow people to specify either ins or outs\n    this.out_sx = typeof opt.out_sx !== 'undefined' ? opt.out_sx : opt.in_sx;\n    this.out_sy = typeof opt.out_sy !== 'undefined' ? opt.out_sy : opt.in_sy;\n    this.out_depth = typeof opt.out_depth !== 'undefined' ? opt.out_depth : opt.in_depth;\n    this.layer_type = 'input';\n  }\n  InputLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      this.out_act = V;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() { },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n\n  global.InputLayer = InputLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // Layers that implement a loss. Currently these are the layers that \n  // can initiate a backward() pass. In future we probably want a more \n  // flexible system that can accomodate multiple losses to do multi-task\n  // learning, and stuff like that. But for now, one of the layers in this\n  // file must be the final layer in a Net.\n\n  // This is a classifier, with N discrete classes from 0 to N-1\n  // it gets a stream of N incoming numbers and computes the softmax\n  // function (exponentiate and normalize to sum to 1 as probabilities should)\n  var SoftmaxLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'softmax';\n  }\n\n  SoftmaxLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = new Vol(1, 1, this.out_depth, 0.0);\n\n      // compute max activation\n      var as = V.w;\n      var amax = V.w[0];\n      for(var i=1;i<this.out_depth;i++) {\n        if(as[i] > amax) amax = as[i];\n      }\n\n      // compute exponentials (carefully to not blow up)\n      var es = global.zeros(this.out_depth);\n      var esum = 0.0;\n      for(var i=0;i<this.out_depth;i++) {\n        var e = Math.exp(as[i] - amax);\n        esum += e;\n        es[i] = e;\n      }\n\n      // normalize and output to sum to one\n      for(var i=0;i<this.out_depth;i++) {\n        es[i] /= esum;\n        A.w[i] = es[i];\n      }\n\n      this.es = es; // save these for backprop\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function(y) {\n\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n\n      for(var i=0;i<this.out_depth;i++) {\n        var indicator = i === y ? 1.0 : 0.0;\n        var mul = -(indicator - this.es[i]);\n        x.dw[i] = mul;\n      }\n\n      // loss is the class negative log likelihood\n      return -Math.log(this.es[y]);\n    },\n    getParamsAndGrads: function() { \n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }\n\n  // implements an L2 regression cost layer,\n  // so penalizes \\sum_i(||x_i - y_i||^2), where x is its input\n  // and y is the user-provided array of \"correct\" values.\n  var RegressionLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'regression';\n  }\n\n  RegressionLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      this.out_act = V;\n      return V; // identity function\n    },\n    // y is a list here of size num_inputs\n    backward: function(y) { \n\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n      var loss = 0.0;\n      if(y instanceof Array || y instanceof Float64Array) {\n        for(var i=0;i<this.out_depth;i++) {\n          var dy = x.w[i] - y[i];\n          x.dw[i] = dy;\n          loss += 2*dy*dy;\n        }\n      } else {\n        // assume it is a struct with entries .dim and .val\n        // and we pass gradient only along dimension dim to be equal to val\n        var i = y.dim;\n        var yi = y.val;\n        var dy = x.w[i] - yi;\n        x.dw[i] = dy;\n        loss += 2*dy*dy;\n      }\n      return loss;\n    },\n    getParamsAndGrads: function() { \n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }\n\n  var SVMLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'svm';\n  }\n\n  SVMLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      this.out_act = V; // nothing to do, output raw scores\n      return V;\n    },\n    backward: function(y) {\n\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n\n      var yscore = x.w[y]; // score of ground truth\n      var margin = 1.0;\n      var loss = 0.0;\n      for(var i=0;i<this.out_depth;i++) {\n        if(-yscore + x.w[i] + margin > 0) {\n          // violating example, apply loss\n          // I love hinge loss, by the way. Truly.\n          // Seriously, compare this SVM code with Softmax forward AND backprop code above\n          // it's clear which one is superior, not only in code, simplicity\n          // and beauty, but also in practice.\n          x.dw[i] += 1;\n          x.dw[y] -= 1;\n          loss += -yscore + x.w[i] + margin;\n        }\n      }\n\n      return loss;\n    },\n    getParamsAndGrads: function() { \n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }\n  \n  global.RegressionLayer = RegressionLayer;\n  global.SoftmaxLayer = SoftmaxLayer;\n  global.SVMLayer = SVMLayer;\n\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // Implements ReLU nonlinearity elementwise\n  // x -> max(0, x)\n  // the output is in [0, inf)\n  var ReluLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'relu';\n  }\n  ReluLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var V2 = V.clone();\n      var N = V.w.length;\n      var V2w = V2.w;\n      for(var i=0;i<N;i++) { \n        if(V2w[i] < 0) V2w[i] = 0; // threshold at 0\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        if(V2.w[i] <= 0) V.dw[i] = 0; // threshold\n        else V.dw[i] = V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n\n  // Implements Sigmoid nnonlinearity elementwise\n  // x -> 1/(1+e^(-x))\n  // so the output is between 0 and 1.\n  var SigmoidLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'sigmoid';\n  }\n  SigmoidLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var V2 = V.cloneAndZero();\n      var N = V.w.length;\n      var V2w = V2.w;\n      var Vw = V.w;\n      for(var i=0;i<N;i++) { \n        V2w[i] = 1.0/(1.0+Math.exp(-Vw[i]));\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        var v2wi = V2.w[i];\n        V.dw[i] =  v2wi * (1.0 - v2wi) * V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n\n  // Implements Maxout nnonlinearity that computes\n  // x -> max(x)\n  // where x is a vector of size group_size. Ideally of course,\n  // the input size should be exactly divisible by group_size\n  var MaxoutLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    this.group_size = typeof opt.group_size !== 'undefined' ? opt.group_size : 2;\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = Math.floor(opt.in_depth / this.group_size);\n    this.layer_type = 'maxout';\n\n    this.switches = global.zeros(this.out_sx*this.out_sy*this.out_depth); // useful for backprop\n  }\n  MaxoutLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var N = this.out_depth; \n      var V2 = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n\n      // optimization branch. If we're operating on 1D arrays we dont have\n      // to worry about keeping track of x,y,d coordinates inside\n      // input volumes. In convnets we do :(\n      if(this.out_sx === 1 && this.out_sy === 1) {\n        for(var i=0;i<N;i++) {\n          var ix = i * this.group_size; // base index offset\n          var a = V.w[ix];\n          var ai = 0;\n          for(var j=1;j<this.group_size;j++) {\n            var a2 = V.w[ix+j];\n            if(a2 > a) {\n              a = a2;\n              ai = j;\n            }\n          }\n          V2.w[i] = a;\n          this.switches[i] = ix + ai;\n        }\n      } else {\n        var n=0; // counter for switches\n        for(var x=0;x<V.sx;x++) {\n          for(var y=0;y<V.sy;y++) {\n            for(var i=0;i<N;i++) {\n              var ix = i * this.group_size;\n              var a = V.get(x, y, ix);\n              var ai = 0;\n              for(var j=1;j<this.group_size;j++) {\n                var a2 = V.get(x, y, ix+j);\n                if(a2 > a) {\n                  a = a2;\n                  ai = j;\n                }\n              }\n              V2.set(x,y,i,a);\n              this.switches[n] = ix + ai;\n              n++;\n            }\n          }\n        }\n\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = this.out_depth;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n\n      // pass the gradient through the appropriate switch\n      if(this.out_sx === 1 && this.out_sy === 1) {\n        for(var i=0;i<N;i++) {\n          var chain_grad = V2.dw[i];\n          V.dw[this.switches[i]] = chain_grad;\n        }\n      } else {\n        // bleh okay, lets do this the hard way\n        var n=0; // counter for switches\n        for(var x=0;x<V2.sx;x++) {\n          for(var y=0;y<V2.sy;y++) {\n            for(var i=0;i<N;i++) {\n              var chain_grad = V2.get_grad(x,y,i);\n              V.set_grad(x,y,this.switches[n],chain_grad);\n              n++;\n            }\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.group_size = this.group_size;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n      this.group_size = json.group_size;\n      this.switches = global.zeros(this.group_size);\n    }\n  }\n\n  // a helper function, since tanh is not yet part of ECMAScript. Will be in v6.\n  function tanh(x) {\n    var y = Math.exp(2 * x);\n    return (y - 1) / (y + 1);\n  }\n  // Implements Tanh nnonlinearity elementwise\n  // x -> tanh(x) \n  // so the output is between -1 and 1.\n  var TanhLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'tanh';\n  }\n  TanhLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var V2 = V.cloneAndZero();\n      var N = V.w.length;\n      for(var i=0;i<N;i++) { \n        V2.w[i] = tanh(V.w[i]);\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        var v2wi = V2.w[i];\n        V.dw[i] = (1.0 - v2wi * v2wi) * V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n  \n  global.TanhLayer = TanhLayer;\n  global.MaxoutLayer = MaxoutLayer;\n  global.ReluLayer = ReluLayer;\n  global.SigmoidLayer = SigmoidLayer;\n\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // An inefficient dropout layer\n  // Note this is not most efficient implementation since the layer before\n  // computed all these activations and now we're just going to drop them :(\n  // same goes for backward pass. Also, if we wanted to be efficient at test time\n  // we could equivalently be clever and upscale during train and copy pointers during test\n  // todo: make more efficient.\n  var DropoutLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'dropout';\n    this.drop_prob = typeof opt.drop_prob !== 'undefined' ? opt.drop_prob : 0.5;\n    this.dropped = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n  }\n  DropoutLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      if(typeof(is_training)==='undefined') { is_training = false; } // default is prediction mode\n      var V2 = V.clone();\n      var N = V.w.length;\n      if(is_training) {\n        // do dropout\n        for(var i=0;i<N;i++) {\n          if(Math.random()<this.drop_prob) { V2.w[i]=0; this.dropped[i] = true; } // drop!\n          else {this.dropped[i] = false;}\n        }\n      } else {\n        // scale the activations during prediction\n        for(var i=0;i<N;i++) { V2.w[i]*=this.drop_prob; }\n      }\n      this.out_act = V2;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var chain_grad = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        if(!(this.dropped[i])) { \n          V.dw[i] = chain_grad.dw[i]; // copy over the gradient\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.drop_prob = this.drop_prob;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n      this.drop_prob = json.drop_prob;\n    }\n  }\n  \n\n  global.DropoutLayer = DropoutLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // a bit experimental layer for now. I think it works but I'm not 100%\n  // the gradient check is a bit funky. I'll look into this a bit later.\n  // Local Response Normalization in window, along depths of volumes\n  var LocalResponseNormalizationLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    this.k = opt.k;\n    this.n = opt.n;\n    this.alpha = opt.alpha;\n    this.beta = opt.beta;\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'lrn';\n\n    // checks\n    if(this.n%2 === 0) { console.log('WARNING n should be odd for LRN layer'); }\n  }\n  LocalResponseNormalizationLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = V.cloneAndZero();\n      this.S_cache_ = V.cloneAndZero();\n      var n2 = Math.floor(this.n/2);\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<V.depth;i++) {\n\n            var ai = V.get(x,y,i);\n\n            // normalize in a window of size n\n            var den = 0.0;\n            for(var j=Math.max(0,i-n2);j<=Math.min(i+n2,V.depth-1);j++) {\n              var aa = V.get(x,y,j);\n              den += aa*aa;\n            }\n            den *= this.alpha / this.n;\n            den += this.k;\n            this.S_cache_.set(x,y,i,den); // will be useful for backprop\n            den = Math.pow(den, this.beta);\n            A.set(x,y,i,ai/den);\n          }\n        }\n      }\n\n      this.out_act = A;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() { \n      // evaluate gradient wrt data\n      var V = this.in_act; // we need to set dw of this\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n      var A = this.out_act; // computed in forward pass \n\n      var n2 = Math.floor(this.n/2);\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<V.depth;i++) {\n\n            var chain_grad = this.out_act.get_grad(x,y,i);\n            var S = this.S_cache_.get(x,y,i);\n            var SB = Math.pow(S, this.beta);\n            var SB2 = SB*SB;\n\n            // normalize in a window of size n\n            for(var j=Math.max(0,i-n2);j<=Math.min(i+n2,V.depth-1);j++) {              \n              var aj = V.get(x,y,j); \n              var g = -aj*this.beta*Math.pow(S,this.beta-1)*this.alpha/this.n*2*aj;\n              if(j===i) g+= SB;\n              g /= SB2;\n              g *= chain_grad;\n              V.add_grad(x,y,j,g);\n            }\n\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() { return []; },\n    toJSON: function() {\n      var json = {};\n      json.k = this.k;\n      json.n = this.n;\n      json.alpha = this.alpha; // normalize by size\n      json.beta = this.beta;\n      json.out_sx = this.out_sx; \n      json.out_sy = this.out_sy;\n      json.out_depth = this.out_depth;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.k = json.k;\n      this.n = json.n;\n      this.alpha = json.alpha; // normalize by size\n      this.beta = json.beta;\n      this.out_sx = json.out_sx; \n      this.out_sy = json.out_sy;\n      this.out_depth = json.out_depth;\n      this.layer_type = json.layer_type;\n    }\n  }\n  \n\n  global.LocalResponseNormalizationLayer = LocalResponseNormalizationLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // transforms x-> [x, x_i*x_j forall i,j]\n  // so the fully connected layer afters will essentially be doing tensor multiplies\n  var QuadTransformLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    // linear terms, and then quadratic terms, of which there are 1/2*n*(n+1),\n    // (offdiagonals and the diagonal total) and arithmetic series.\n    // Actually never mind, lets not be fancy here yet and just include\n    // terms x_ix_j and x_jx_i twice. Half as efficient but much less\n    // headache.\n    this.out_depth = opt.in_depth + opt.in_depth * opt.in_depth;\n    this.layer_type = 'quadtransform';\n\n  }\n  QuadTransformLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var N = this.out_depth;\n      var Ni = V.depth;\n      var V2 = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<N;i++) {\n            if(i<Ni) {\n              V2.set(x,y,i,V.get(x,y,i)); // copy these over (linear terms)\n            } else {\n              var i0 = Math.floor((i-Ni)/Ni);\n              var i1 = (i-Ni) - i0*Ni;\n              V2.set(x,y,i,V.get(x,y,i0) * V.get(x,y,i1)); // quadratic\n            }\n          }\n        }\n      }\n      this.out_act = V2;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() {\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n      var V2 = this.out_act;\n      var N = this.out_depth;\n      var Ni = V.depth;\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<N;i++) {\n            var chain_grad = V2.get_grad(x,y,i);\n            if(i<Ni) {\n              V.add_grad(x,y,i,chain_grad);\n            } else {\n              var i0 = Math.floor((i-Ni)/Ni);\n              var i1 = (i-Ni) - i0*Ni;\n              V.add_grad(x,y,i0,V.get(x,y,i1)*chain_grad);\n              V.add_grad(x,y,i1,V.get(x,y,i0)*chain_grad);\n            }\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n  \n\n  global.QuadTransformLayer = QuadTransformLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // Net manages a set of layers\n  // For now constraints: Simple linear order of layers, first layer input last layer a cost layer\n  var Net = function(options) {\n    this.layers = [];\n  }\n\n  Net.prototype = {\n    \n    // takes a list of layer definitions and creates the network layer objects\n    makeLayers: function(defs) {\n\n      // few checks for now\n      if(defs.length<2) {console.log('ERROR! For now at least have input and softmax layers.');}\n      if(defs[0].type !== 'input') {console.log('ERROR! For now first layer should be input.');}\n\n      // desugar syntactic for adding activations and dropouts\n      var desugar = function() {\n        var new_defs = [];\n        for(var i=0;i<defs.length;i++) {\n          var def = defs[i];\n          \n          if(def.type==='softmax' || def.type==='svm') {\n            // add an fc layer here, there is no reason the user should\n            // have to worry about this and we almost always want to\n            new_defs.push({type:'fc', num_neurons: def.num_classes});\n          }\n\n          if(def.type==='regression') {\n            // add an fc layer here, there is no reason the user should\n            // have to worry about this and we almost always want to\n            new_defs.push({type:'fc', num_neurons: def.num_neurons});\n          }\n\n          if((def.type==='fc' || def.type==='conv') \n              && typeof(def.bias_pref) === 'undefined'){\n            def.bias_pref = 0.0;\n            if(typeof def.activation !== 'undefined' && def.activation === 'relu') {\n              def.bias_pref = 0.1; // relus like a bit of positive bias to get gradients early\n              // otherwise it's technically possible that a relu unit will never turn on (by chance)\n              // and will never get any gradient and never contribute any computation. Dead relu.\n            }\n          }\n          \n          if(typeof def.tensor !== 'undefined') {\n            // apply quadratic transform so that the upcoming multiply will include\n            // quadratic terms, equivalent to doing a tensor product\n            if(def.tensor) {\n              new_defs.push({type: 'quadtransform'});\n            }\n          }\n\n          new_defs.push(def);\n\n          if(typeof def.activation !== 'undefined') {\n            if(def.activation==='relu') { new_defs.push({type:'relu'}); }\n            else if (def.activation==='sigmoid') { new_defs.push({type:'sigmoid'}); }\n            else if (def.activation==='tanh') { new_defs.push({type:'tanh'}); }\n            else if (def.activation==='maxout') {\n              // create maxout activation, and pass along group size, if provided\n              var gs = def.group_size !== 'undefined' ? def.group_size : 2;\n              new_defs.push({type:'maxout', group_size:gs});\n            }\n            else { console.log('ERROR unsupported activation ' + def.activation); }\n          }\n          if(typeof def.drop_prob !== 'undefined' && def.type !== 'dropout') {\n            new_defs.push({type:'dropout', drop_prob: def.drop_prob});\n          }\n\n        }\n        return new_defs;\n      }\n      defs = desugar(defs);\n\n      // create the layers\n      this.layers = [];\n      for(var i=0;i<defs.length;i++) {\n        var def = defs[i];\n        if(i>0) {\n          var prev = this.layers[i-1];\n          def.in_sx = prev.out_sx;\n          def.in_sy = prev.out_sy;\n          def.in_depth = prev.out_depth;\n        }\n\n        switch(def.type) {\n          case 'fc': this.layers.push(new global.FullyConnLayer(def)); break;\n          case 'lrn': this.layers.push(new global.LocalResponseNormalizationLayer(def)); break;\n          case 'dropout': this.layers.push(new global.DropoutLayer(def)); break;\n          case 'input': this.layers.push(new global.InputLayer(def)); break;\n          case 'softmax': this.layers.push(new global.SoftmaxLayer(def)); break;\n          case 'regression': this.layers.push(new global.RegressionLayer(def)); break;\n          case 'conv': this.layers.push(new global.ConvLayer(def)); break;\n          case 'pool': this.layers.push(new global.PoolLayer(def)); break;\n          case 'relu': this.layers.push(new global.ReluLayer(def)); break;\n          case 'sigmoid': this.layers.push(new global.SigmoidLayer(def)); break;\n          case 'tanh': this.layers.push(new global.TanhLayer(def)); break;\n          case 'maxout': this.layers.push(new global.MaxoutLayer(def)); break;\n          case 'quadtransform': this.layers.push(new global.QuadTransformLayer(def)); break;\n          case 'svm': this.layers.push(new global.SVMLayer(def)); break;\n          default: console.log('ERROR: UNRECOGNIZED LAYER TYPE!');\n        }\n      }\n    },\n\n    // forward prop the network. A trainer will pass in is_training = true\n    forward: function(V, is_training) {\n      if(typeof(is_training)==='undefined') is_training = false;\n      var act = this.layers[0].forward(V, is_training);\n      for(var i=1;i<this.layers.length;i++) {\n        act = this.layers[i].forward(act, is_training);\n      }\n      return act;\n    },\n    \n    // backprop: compute gradients wrt all parameters\n    backward: function(y) {\n      var N = this.layers.length;\n      var loss = this.layers[N-1].backward(y); // last layer assumed softmax\n      for(var i=N-2;i>=0;i--) { // first layer assumed input\n        this.layers[i].backward();\n      }\n      return loss;\n    },\n    getParamsAndGrads: function() {\n      // accumulate parameters and gradients for the entire network\n      var response = [];\n      for(var i=0;i<this.layers.length;i++) {\n        var layer_reponse = this.layers[i].getParamsAndGrads();\n        for(var j=0;j<layer_reponse.length;j++) {\n          response.push(layer_reponse[j]);\n        }\n      }\n      return response;\n    },\n    getPrediction: function() {\n      var S = this.layers[this.layers.length-1]; // softmax layer\n      var p = S.out_act.w;\n      var maxv = p[0];\n      var maxi = 0;\n      for(var i=1;i<p.length;i++) {\n        if(p[i] > maxv) { maxv = p[i]; maxi = i;}\n      }\n      return maxi;\n    },\n    toJSON: function() {\n      var json = {};\n      json.layers = [];\n      for(var i=0;i<this.layers.length;i++) {\n        json.layers.push(this.layers[i].toJSON());\n      }\n      return json;\n    },\n    fromJSON: function(json) {\n      this.layers = [];\n      for(var i=0;i<json.layers.length;i++) {\n        var Lj = json.layers[i]\n        var t = Lj.layer_type;\n        var L;\n        if(t==='input') { L = new global.InputLayer(); }\n        if(t==='relu') { L = new global.ReluLayer(); }\n        if(t==='sigmoid') { L = new global.SigmoidLayer(); }\n        if(t==='tanh') { L = new global.TanhLayer(); }\n        if(t==='dropout') { L = new global.DropoutLayer(); }\n        if(t==='conv') { L = new global.ConvLayer(); }\n        if(t==='pool') { L = new global.PoolLayer(); }\n        if(t==='lrn') { L = new global.LocalResponseNormalizationLayer(); }\n        if(t==='softmax') { L = new global.SoftmaxLayer(); }\n        if(t==='regression') { L = new global.RegressionLayer(); }\n        if(t==='fc') { L = new global.FullyConnLayer(); }\n        if(t==='maxout') { L = new global.MaxoutLayer(); }\n        if(t==='quadtransform') { L = new global.QuadTransformLayer(); }\n        if(t==='svm') { L = new global.SVMLayer(); }\n        L.fromJSON(Lj);\n        this.layers.push(L);\n      }\n    }\n  }\n  \n\n  global.Net = Net;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  var Trainer = function(net, options) {\n\n    this.net = net;\n\n    var options = options || {};\n    this.learning_rate = typeof options.learning_rate !== 'undefined' ? options.learning_rate : 0.01;\n    this.l1_decay = typeof options.l1_decay !== 'undefined' ? options.l1_decay : 0.0;\n    this.l2_decay = typeof options.l2_decay !== 'undefined' ? options.l2_decay : 0.0;\n    this.batch_size = typeof options.batch_size !== 'undefined' ? options.batch_size : 1;\n    this.method = typeof options.method !== 'undefined' ? options.method : 'sgd'; // sgd/adagrad/adadelta/windowgrad\n\n    this.momentum = typeof options.momentum !== 'undefined' ? options.momentum : 0.9;\n    this.ro = typeof options.ro !== 'undefined' ? options.ro : 0.95; // used in adadelta\n    this.eps = typeof options.eps !== 'undefined' ? options.eps : 1e-6; // used in adadelta\n\n    this.k = 0; // iteration counter\n    this.gsum = []; // last iteration gradients (used for momentum calculations)\n    this.xsum = []; // used in adadelta\n  }\n\n  Trainer.prototype = {\n    train: function(x, y) {\n\n      var start = new Date().getTime();\n      this.net.forward(x, true); // also set the flag that lets the net know we're just training\n      var end = new Date().getTime();\n      var fwd_time = end - start;\n\n      var start = new Date().getTime();\n      var cost_loss = this.net.backward(y);\n      var l2_decay_loss = 0.0;\n      var l1_decay_loss = 0.0;\n      var end = new Date().getTime();\n      var bwd_time = end - start;\n      \n      this.k++;\n      if(this.k % this.batch_size === 0) {\n\n        var pglist = this.net.getParamsAndGrads();\n\n        // initialize lists for accumulators. Will only be done once on first iteration\n        if(this.gsum.length === 0 && (this.method !== 'sgd' || this.momentum > 0.0)) {\n          // only vanilla sgd doesnt need either lists\n          // momentum needs gsum\n          // adagrad needs gsum\n          // adadelta needs gsum and xsum\n          for(var i=0;i<pglist.length;i++) {\n            this.gsum.push(global.zeros(pglist[i].params.length));\n            if(this.method === 'adadelta') {\n              this.xsum.push(global.zeros(pglist[i].params.length));\n            } else {\n              this.xsum.push([]); // conserve memory\n            }\n          }\n        }\n\n        // perform an update for all sets of weights\n        for(var i=0;i<pglist.length;i++) {\n          var pg = pglist[i]; // param, gradient, other options in future (custom learning rate etc)\n          var p = pg.params;\n          var g = pg.grads;\n\n          // learning rate for some parameters.\n          var l2_decay_mul = typeof pg.l2_decay_mul !== 'undefined' ? pg.l2_decay_mul : 1.0;\n          var l1_decay_mul = typeof pg.l1_decay_mul !== 'undefined' ? pg.l1_decay_mul : 1.0;\n          var l2_decay = this.l2_decay * l2_decay_mul;\n          var l1_decay = this.l1_decay * l1_decay_mul;\n\n          var plen = p.length;\n          for(var j=0;j<plen;j++) {\n            l2_decay_loss += l2_decay*p[j]*p[j]/2; // accumulate weight decay loss\n            l1_decay_loss += l1_decay*Math.abs(p[j]);\n            var l1grad = l1_decay * (p[j] > 0 ? 1 : -1);\n            var l2grad = l2_decay * (p[j]);\n\n            var gij = (l2grad + l1grad + g[j]) / this.batch_size; // raw batch gradient\n\n            var gsumi = this.gsum[i];\n            var xsumi = this.xsum[i];\n            if(this.method === 'adagrad') {\n              // adagrad update\n              gsumi[j] = gsumi[j] + gij * gij;\n              var dx = - this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij;\n              p[j] += dx;\n            } else if(this.method === 'windowgrad') {\n              // this is adagrad but with a moving window weighted average\n              // so the gradient is not accumulated over the entire history of the run. \n              // it's also referred to as Idea #1 in Zeiler paper on Adadelta. Seems reasonable to me!\n              gsumi[j] = this.ro * gsumi[j] + (1-this.ro) * gij * gij;\n              var dx = - this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij; // eps added for better conditioning\n              p[j] += dx;\n            } else if(this.method === 'adadelta') {\n              // assume adadelta if not sgd or adagrad\n              gsumi[j] = this.ro * gsumi[j] + (1-this.ro) * gij * gij;\n              var dx = - Math.sqrt((xsumi[j] + this.eps)/(gsumi[j] + this.eps)) * gij;\n              xsumi[j] = this.ro * xsumi[j] + (1-this.ro) * dx * dx; // yes, xsum lags behind gsum by 1.\n              p[j] += dx;\n            } else {\n              // assume SGD\n              if(this.momentum > 0.0) {\n                // momentum update\n                var dx = this.momentum * gsumi[j] - this.learning_rate * gij; // step\n                gsumi[j] = dx; // back this up for next iteration of momentum\n                p[j] += dx; // apply corrected gradient\n              } else {\n                // vanilla sgd\n                p[j] +=  - this.learning_rate * gij;\n              }\n            }\n            g[j] = 0.0; // zero out gradient so that we can begin accumulating anew\n          }\n        }\n      }\n\n      // appending softmax_loss for backwards compatibility, but from now on we will always use cost_loss\n      // in future, TODO: have to completely redo the way loss is done around the network as currently \n      // loss is a bit of a hack. Ideally, user should specify arbitrary number of loss functions on any layer\n      // and it should all be computed correctly and automatically. \n      return {fwd_time: fwd_time, bwd_time: bwd_time, \n              l2_decay_loss: l2_decay_loss, l1_decay_loss: l1_decay_loss,\n              cost_loss: cost_loss, softmax_loss: cost_loss, \n              loss: cost_loss + l1_decay_loss + l2_decay_loss}\n    }\n  }\n  \n  global.Trainer = Trainer;\n  global.SGDTrainer = Trainer; // backwards compatibility\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n\n  // used utilities, make explicit local references\n  var randf = global.randf;\n  var randi = global.randi;\n  var Net = global.Net;\n  var Trainer = global.Trainer;\n  var maxmin = global.maxmin;\n  var randperm = global.randperm;\n  var weightedSample = global.weightedSample;\n  var getopt = global.getopt;\n  var arrUnique = global.arrUnique;\n\n  /*\n  A MagicNet takes data: a list of convnetjs.Vol(), and labels\n  which for now are assumed to be class indeces 0..K. MagicNet then:\n  - creates data folds for cross-validation\n  - samples candidate networks\n  - evaluates candidate networks on all data folds\n  - produces predictions by model-averaging the best networks\n  */\n  var MagicNet = function(data, labels, opt) {\n    var opt = opt || {};\n    if(typeof data === 'undefined') { data = []; }\n    if(typeof labels === 'undefined') { labels = []; }\n\n    // required inputs\n    this.data = data; // store these pointers to data\n    this.labels = labels;\n\n    // optional inputs\n    this.train_ratio = getopt(opt, 'train_ratio', 0.7);\n    this.num_folds = getopt(opt, 'num_folds', 10);\n    this.num_candidates = getopt(opt, 'num_candidates', 50); // we evaluate several in parallel\n    // how many epochs of data to train every network? for every fold?\n    // higher values mean higher accuracy in final results, but more expensive\n    this.num_epochs = getopt(opt, 'num_epochs', 50); \n    // number of best models to average during prediction. Usually higher = better\n    this.ensemble_size = getopt(opt, 'ensemble_size', 10);\n\n    // candidate parameters\n    this.batch_size_min = getopt(opt, 'batch_size_min', 10);\n    this.batch_size_max = getopt(opt, 'batch_size_max', 300);\n    this.l2_decay_min = getopt(opt, 'l2_decay_min', -4);\n    this.l2_decay_max = getopt(opt, 'l2_decay_max', 2);\n    this.learning_rate_min = getopt(opt, 'learning_rate_min', -4);\n    this.learning_rate_max = getopt(opt, 'learning_rate_max', 0);\n    this.momentum_min = getopt(opt, 'momentum_min', 0.9);\n    this.momentum_max = getopt(opt, 'momentum_max', 0.9);\n    this.neurons_min = getopt(opt, 'neurons_min', 5);\n    this.neurons_max = getopt(opt, 'neurons_max', 30);\n\n    // computed\n    this.folds = []; // data fold indices, gets filled by sampleFolds()\n    this.candidates = []; // candidate networks that are being currently evaluated\n    this.evaluated_candidates = []; // history of all candidates that were fully evaluated on all folds\n    this.unique_labels = arrUnique(labels);\n    this.iter = 0; // iteration counter, goes from 0 -> num_epochs * num_training_data\n    this.foldix = 0; // index of active fold\n\n    // callbacks\n    this.finish_fold_callback = null;\n    this.finish_batch_callback = null;\n\n    // initializations\n    if(this.data.length > 0) {\n      this.sampleFolds();\n      this.sampleCandidates();\n    }\n  };\n\n  MagicNet.prototype = {\n\n    // sets this.folds to a sampling of this.num_folds folds\n    sampleFolds: function() {\n      var N = this.data.length;\n      var num_train = Math.floor(this.train_ratio * N);\n      this.folds = []; // flush folds, if any\n      for(var i=0;i<this.num_folds;i++) {\n        var p = randperm(N);\n        this.folds.push({train_ix: p.slice(0, num_train), test_ix: p.slice(num_train, N)});\n      }\n    },\n\n    // returns a random candidate network\n    sampleCandidate: function() {\n      var input_depth = this.data[0].w.length;\n      var num_classes = this.unique_labels.length;\n\n      // sample network topology and hyperparameters\n      var layer_defs = [];\n      layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth: input_depth});\n      var nl = weightedSample([0,1,2,3], [0.2, 0.3, 0.3, 0.2]); // prefer nets with 1,2 hidden layers\n      for(var q=0;q<nl;q++) {\n        var ni = randi(this.neurons_min, this.neurons_max);\n        var act = ['tanh','maxout','relu'][randi(0,3)];\n        if(randf(0,1)<0.5) {\n          var dp = Math.random();\n          layer_defs.push({type:'fc', num_neurons: ni, activation: act, drop_prob: dp});\n        } else {\n          layer_defs.push({type:'fc', num_neurons: ni, activation: act});\n        }\n      }\n      layer_defs.push({type:'softmax', num_classes: num_classes});\n      var net = new Net();\n      net.makeLayers(layer_defs);\n\n      // sample training hyperparameters\n      var bs = randi(this.batch_size_min, this.batch_size_max); // batch size\n      var l2 = Math.pow(10, randf(this.l2_decay_min, this.l2_decay_max)); // l2 weight decay\n      var lr = Math.pow(10, randf(this.learning_rate_min, this.learning_rate_max)); // learning rate\n      var mom = randf(this.momentum_min, this.momentum_max); // momentum. Lets just use 0.9, works okay usually ;p\n      var tp = randf(0,1); // trainer type\n      var trainer_def;\n      if(tp<0.33) {\n        trainer_def = {method:'adadelta', batch_size:bs, l2_decay:l2};\n      } else if(tp<0.66) {\n        trainer_def = {method:'adagrad', learning_rate: lr, batch_size:bs, l2_decay:l2};\n      } else {\n        trainer_def = {method:'sgd', learning_rate: lr, momentum: mom, batch_size:bs, l2_decay:l2};\n      }\n      \n      var trainer = new Trainer(net, trainer_def);\n\n      var cand = {};\n      cand.acc = [];\n      cand.accv = 0; // this will maintained as sum(acc) for convenience\n      cand.layer_defs = layer_defs;\n      cand.trainer_def = trainer_def;\n      cand.net = net;\n      cand.trainer = trainer;\n      return cand;\n    },\n\n    // sets this.candidates with this.num_candidates candidate nets\n    sampleCandidates: function() {\n      this.candidates = []; // flush, if any\n      for(var i=0;i<this.num_candidates;i++) {\n        var cand = this.sampleCandidate();\n        this.candidates.push(cand);\n      }\n    },\n\n    step: function() {\n      \n      // run an example through current candidate\n      this.iter++;\n\n      // step all candidates on a random data point\n      var fold = this.folds[this.foldix]; // active fold\n      var dataix = fold.train_ix[randi(0, fold.train_ix.length)];\n      for(var k=0;k<this.candidates.length;k++) {\n        var x = this.data[dataix];\n        var l = this.labels[dataix];\n        this.candidates[k].trainer.train(x, l);\n      }\n\n      // process consequences: sample new folds, or candidates\n      var lastiter = this.num_epochs * fold.train_ix.length;\n      if(this.iter >= lastiter) {\n        // finished evaluation of this fold. Get final validation\n        // accuracies, record them, and go on to next fold.\n        var val_acc = this.evalValErrors();\n        for(var k=0;k<this.candidates.length;k++) {\n          var c = this.candidates[k];\n          c.acc.push(val_acc[k]);\n          c.accv += val_acc[k];\n        }\n        this.iter = 0; // reset step number\n        this.foldix++; // increment fold\n\n        if(this.finish_fold_callback !== null) {\n          this.finish_fold_callback();\n        }\n\n        if(this.foldix >= this.folds.length) {\n          // we finished all folds as well! Record these candidates\n          // and sample new ones to evaluate.\n          for(var k=0;k<this.candidates.length;k++) {\n            this.evaluated_candidates.push(this.candidates[k]);\n          }\n          // sort evaluated candidates according to accuracy achieved\n          this.evaluated_candidates.sort(function(a, b) { \n            return (a.accv / a.acc.length) \n                 > (b.accv / b.acc.length) \n                 ? -1 : 1;\n          });\n          // and clip only to the top few ones (lets place limit at 3*ensemble_size)\n          // otherwise there are concerns with keeping these all in memory \n          // if MagicNet is being evaluated for a very long time\n          if(this.evaluated_candidates.length > 3 * this.ensemble_size) {\n            this.evaluated_candidates = this.evaluated_candidates.slice(0, 3 * this.ensemble_size);\n          }\n          if(this.finish_batch_callback !== null) {\n            this.finish_batch_callback();\n          }\n          this.sampleCandidates(); // begin with new candidates\n          this.foldix = 0; // reset this\n        } else {\n          // we will go on to another fold. reset all candidates nets\n          for(var k=0;k<this.candidates.length;k++) {\n            var c = this.candidates[k];\n            var net = new Net();\n            net.makeLayers(c.layer_defs);\n            var trainer = new Trainer(net, c.trainer_def);\n            c.net = net;\n            c.trainer = trainer;\n          }\n        }\n      }\n    },\n\n    evalValErrors: function() {\n      // evaluate candidates on validation data and return performance of current networks\n      // as simple list\n      var vals = [];\n      var fold = this.folds[this.foldix]; // active fold\n      for(var k=0;k<this.candidates.length;k++) {\n        var net = this.candidates[k].net;\n        var v = 0.0;\n        for(var q=0;q<fold.test_ix.length;q++) {\n          var x = this.data[fold.test_ix[q]];\n          var l = this.labels[fold.test_ix[q]];\n          net.forward(x);\n          var yhat = net.getPrediction();\n          v += (yhat === l ? 1.0 : 0.0); // 0 1 loss\n        }\n        v /= fold.test_ix.length; // normalize\n        vals.push(v);\n      }\n      return vals;\n    },\n\n    // returns prediction scores for given test data point, as Vol\n    // uses an averaged prediction from the best ensemble_size models\n    // x is a Vol.\n    predict_soft: function(data) {\n      // forward prop the best networks\n      // and accumulate probabilities at last layer into a an output Vol\n      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);\n      if(nv === 0) { return new convnetjs.Vol(0,0,0); } // not sure what to do here? we're not ready yet\n      var xout, n;\n      for(var j=0;j<nv;j++) {\n        var net = this.evaluated_candidates[j].net;\n        var x = net.forward(data);\n        if(j===0) { \n          xout = x; \n          n = x.w.length; \n        } else {\n          // add it on\n          for(var d=0;d<n;d++) {\n            xout.w[d] += x.w[d];\n          }\n        }\n      }\n      // produce average\n      for(var d=0;d<n;d++) {\n        xout.w[d] /= n;\n      }\n      return xout;\n    },\n\n    predict: function(data) {\n      var xout = this.predict_soft(data);\n      if(xout.w.length !== 0) {\n        var stats = maxmin(xout.w);\n        var predicted_label = stats.maxi; \n      } else {\n        var predicted_label = -1; // error out\n      }\n      return predicted_label;\n\n    },\n\n    toJSON: function() {\n      // dump the top ensemble_size networks as a list\n      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);\n      var json = {};\n      json.nets = [];\n      for(var i=0;i<nv;i++) {\n        json.nets.push(this.evaluated_candidates[i].net.toJSON());\n      }\n      return json;\n    },\n\n    fromJSON: function(json) {\n      this.ensemble_size = json.nets.length;\n      this.evaluated_candidates = [];\n      for(var i=0;i<this.ensemble_size;i++) {\n        var net = new Net();\n        net.fromJSON(json.nets[i]);\n        var dummy_candidate = {};\n        dummy_candidate.net = net;\n        this.evaluated_candidates.push(dummy_candidate);\n      }\n    },\n\n    // callback functions\n    // called when a fold is finished, while evaluating a batch\n    onFinishFold: function(f) { this.finish_fold_callback = f; },\n    // called when a batch of candidates has finished evaluating\n    onFinishBatch: function(f) { this.finish_batch_callback = f; }\n    \n  };\n\n  global.MagicNet = MagicNet;\n})(convnetjs);\n(function(lib) {\n  \"use strict\";\n  if (typeof module === \"undefined\" || typeof module.exports === \"undefined\") {\n    window.jsfeat = lib; // in ordinary browser attach library to window\n  } else {\n    module.exports = lib; // in nodejs\n  }\n})(convnetjs);\n","export var RANGE_TRAINING_DATA = [-5, 5];\nexport var RANGE_NUM_NEURONS = [1, 5];\nexport var SIZE_PREVIEW_DIAGRAM = 50;\nexport var WIDTH_PREVIEW_DIAGRAM = 250;\nexport var DEFAULT_LAYER_INFO = {\n    input: 2,\n    hiddens: [4, 4],\n    output: 2,\n};\n","import * as convnetjs from 'convnetjs';\nimport { RANGE_TRAINING_DATA, SIZE_PREVIEW_DIAGRAM } from '../../defs';\nvar ctx = self;\nvar sendMessage = self.postMessage;\nctx.addEventListener('message', function (message) {\n    var receive_data = JSON.parse(message.data);\n    var type = receive_data.type;\n    var send_data = {};\n    switch (type) {\n        case 'train':\n            var train_net = new convnetjs.Net();\n            train_net.fromJSON(receive_data.model);\n            var input_array = receive_data.input;\n            var output_array = receive_data.output;\n            var train_max = receive_data.max;\n            var train_min_1 = receive_data.min;\n            var train_length_1 = [];\n            train_max.forEach(function (item, index) {\n                train_length_1[index] = item - train_min_1[index];\n            });\n            var dimension = input_array.length;\n            var x = new convnetjs.Vol(1, 1, dimension);\n            var trainer = new convnetjs.Trainer(train_net, { method: 'adagrad', l2_decay: 0.001, l1_decay: 0.001, batch_size: 20 });\n            for (var loop_time = 0; loop_time < 20; loop_time++) {\n                for (var i = 0; i < input_array.length; i++) {\n                    var train_data = [];\n                    for (var j = 0; j < input_array[i].length; j++) {\n                        if (train_length_1[j] != 0) {\n                            train_data[j] = (input_array[i][j] - train_min_1[j]) / train_length_1[j] * 10 - 5;\n                        }\n                        else {\n                            train_data[j] = input_array[i][j];\n                        }\n                    }\n                    x.w = train_data;\n                    trainer.train(x, output_array[i]);\n                }\n            }\n            send_data['net'] = train_net.toJSON();\n            send_data['type'] = 'train';\n            send_data['id'] = receive_data.id;\n            sendMessage(send_data);\n            break;\n        case 'draw':\n            var draw_net = new convnetjs.Net();\n            draw_net.fromJSON(receive_data.model);\n            var step = (RANGE_TRAINING_DATA[1] - RANGE_TRAINING_DATA[0]) / SIZE_PREVIEW_DIAGRAM;\n            var draw_data = [];\n            var num_line = 0;\n            for (var dx = RANGE_TRAINING_DATA[0]; dx < RANGE_TRAINING_DATA[1]; dx = Number((dx + step).toFixed(1))) {\n                draw_data[num_line] = [];\n                var _loop_1 = function (dy) {\n                    var test_node = new convnetjs.Vol([dx, dy]);\n                    var scores = draw_net.forward(test_node);\n                    var main_class = 0;\n                    var benchmark = 1 / scores.w.length;\n                    var density = benchmark;\n                    if (receive_data.is_simple_mode) {\n                        density = 0.8;\n                    }\n                    else {\n                        scores.w.forEach(function (weight, class_index) {\n                            if (weight > benchmark) {\n                                main_class = class_index;\n                                density = weight;\n                            }\n                        });\n                        density = (density - benchmark) / (1 - benchmark);\n                    }\n                    draw_data[num_line].unshift({\n                        class_index: main_class,\n                        density: density,\n                    });\n                };\n                for (var dy = RANGE_TRAINING_DATA[0]; dy < RANGE_TRAINING_DATA[1]; dy = Number((dy + step).toFixed(1))) {\n                    _loop_1(dy);\n                }\n                num_line++;\n            }\n            send_data['output_data'] = draw_data;\n            send_data['type'] = 'draw';\n            send_data['id'] = receive_data.id;\n            sendMessage(send_data);\n            break;\n    }\n});\n"],"sourceRoot":""}